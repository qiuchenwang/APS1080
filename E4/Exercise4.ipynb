{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r99_JvEI-kvM"
      },
      "source": [
        "# Exercise IV: Function Approximation\n",
        ">You will use function approximation to device controllers for the mountain car and/or cart pole problems in Assignment IV.\n",
        "\n",
        ">Rather than code a function approximation class (straightforward but outside of the scope of our course), you'll use a library, TensorFlow, for this.\n",
        "\n",
        ">There are two front ends for TensorFlow that make it easier to construct, debug, etc., neural networks (general nonlinear function approximation structures): Keras (keras.io) and TfLearn (tflearn.org).\n",
        "\n",
        ">Consult the documentation for these two (either, your choice) and learn how to:\n",
        "\n",
        ">* Construct a neural network with several layers\n",
        ">* Obtain the gradient of the neural network with respect to its parameters\n",
        ">* Train the neural network by stochastic gradient descent using a loop that you construct and in which you update the weights as per the gradient and the error (you will use this in A-IV)\n",
        "\n",
        ">Submit, via pdf, the results of your investigation. Graded for completion; this is an opportunity for you to ask questions in case you are stuck.\n",
        "\n",
        ">Consult Files/stochastic_gradient_descent_example.py for one example to help you get started.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EO3f4hf-fjs"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "#from keras.optimizers import Adam"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKCFUVeyG3iX"
      },
      "source": [
        "##Carte Pole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCLHIljHB3dh",
        "outputId": "42033a64-6694-4cf0-e815-57808418b478"
      },
      "source": [
        "#Set up the environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done)\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "    \n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step i 0 action= 0\n",
            "obs= [-0.02640279 -0.20614888  0.0288152   0.26124097] reward= 1.0 done= False\n",
            "step i 1 action= 1\n",
            "obs= [-0.03052577 -0.01144986  0.03404002 -0.02221583] reward= 1.0 done= False\n",
            "step i 2 action= 1\n",
            "obs= [-0.03075477  0.18316781  0.0335957  -0.30396738] reward= 1.0 done= False\n",
            "step i 3 action= 0\n",
            "obs= [-0.02709141 -0.01241642  0.02751636 -0.00088131] reward= 1.0 done= False\n",
            "step i 4 action= 0\n",
            "obs= [-0.02733974 -0.20792196  0.02749873  0.30035479] reward= 1.0 done= False\n",
            "step i 5 action= 1\n",
            "obs= [-0.03149818 -0.01320254  0.03350582  0.01646968] reward= 1.0 done= False\n",
            "step i 6 action= 0\n",
            "obs= [-0.03176223 -0.20878858  0.03383522  0.31953296] reward= 1.0 done= False\n",
            "step i 7 action= 1\n",
            "obs= [-0.035938   -0.01416444  0.04022588  0.03770948] reward= 1.0 done= False\n",
            "step i 8 action= 0\n",
            "obs= [-0.03622129 -0.20983945  0.04098007  0.34280779] reward= 1.0 done= False\n",
            "step i 9 action= 0\n",
            "obs= [-0.04041808 -0.40551973  0.04783622  0.64812654] reward= 1.0 done= False\n",
            "step i 10 action= 0\n",
            "obs= [-0.04852847 -0.60127433  0.06079875  0.95548103] reward= 1.0 done= False\n",
            "step i 11 action= 1\n",
            "obs= [-0.06055396 -0.40702053  0.07990837  0.68250254] reward= 1.0 done= False\n",
            "step i 12 action= 1\n",
            "obs= [-0.06869437 -0.21309385  0.09355843  0.4160088 ] reward= 1.0 done= False\n",
            "step i 13 action= 0\n",
            "obs= [-0.07295625 -0.40940861  0.1018786   0.73665916] reward= 1.0 done= False\n",
            "step i 14 action= 0\n",
            "obs= [-0.08114442 -0.60577909  0.11661178  1.0595881 ] reward= 1.0 done= False\n",
            "step i 15 action= 0\n",
            "obs= [-0.09326    -0.80223628  0.13780355  1.38647875] reward= 1.0 done= False\n",
            "step i 16 action= 1\n",
            "obs= [-0.10930473 -0.60907478  0.16553312  1.13986937] reward= 1.0 done= False\n",
            "step i 17 action= 1\n",
            "obs= [-0.12148622 -0.41645749  0.18833051  0.90333765] reward= 1.0 done= False\n",
            "step i 18 action= 1\n",
            "obs= [-0.12981537 -0.2243168   0.20639726  0.67526516] reward= 1.0 done= False\n",
            "step i 19 action= 0\n",
            "obs= [-0.13430171 -0.42161795  0.21990257  1.02518637] reward= 1.0 done= True\n",
            "Iterations that were run: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3-ICpfG_j0"
      },
      "source": [
        "##Training: Nerual Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMrvgIevlutx"
      },
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self, state_dim, action_size):\n",
        "        self.state_hist = np.zeros((self.state_dim, *action_size), dtype = np.float32)\n",
        "        self.action_hist = np.zeros(self.state_dim, dtype = np.int32)\n",
        "        self.new_state_hist = np.zeros((self.state_dim, *action_size), dtype = np.float32)\n",
        "        self.reward_hist = np.zeros(self.state_dim, dtype = np.float32)\n",
        "        self.done_hist = np.zeros(self.state_dim, dtype=np.uint8)\n",
        "    \n",
        "    def update_values(self,):\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "       \n",
        "\n",
        "\n",
        "        #self.state_in = tf.placeholder(tf.float32, shape=[None, *state_dim])\n",
        "        #self.action_in = tf.placeholder(tf.int32, shape=[None])\n",
        "        self.q_target_in = tf.placeholder(tf.float32, shape=[None])\n",
        "        action_one_hot = tf.one_hot(self.action_in, depth=action_size)\n",
        "        \n",
        "        #self.hidden1 = tf.layers.dense(self.state_in, 100, activation=tf.nn.relu)\n",
        "        #self.q_state = tf.layers.dense(self.hidden1, action_size, activation=None)\n",
        "        #self.q_state_action = tf.reduce_sum(tf.multiply(self.q_state, action_one_hot), axis=1)\n",
        "        \n",
        "        #self.loss = tf.reduce_mean(tf.square(self.q_state_action - self.q_target_in))\n",
        "        #self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss)\n",
        "        \n",
        "    def update_model(self, session, state, action, q_target):\n",
        "        feed = {self.state_in: state, self.action_in: action, self.q_target_in: q_target}\n",
        "        session.run(self.optimizer, feed_dict=feed)\n",
        "        \n",
        "    def get_q_state(self, session, state):\n",
        "        q_state = session.run(self.q_state, feed_dict={self.state_in: state})\n",
        "        return q_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrV_PimqbRCM"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size, input_shape):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, terminal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoqhUCvts_Rk"
      },
      "source": [
        "def QNetwork(input_dims, action_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32,input_shape=(*input_dims,), activation=\"relu\"))\n",
        "    model.add(Dense(64, activation=\"relu\"))\n",
        "    model.add(Dense(action_size, activation=\"linear\"))\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=alpha))\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8SDAFZm3x7k"
      },
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, max_len):\n",
        "        self.buffer = deque(max_len = max_len)\n",
        "        \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        sample_size = min(len(self.buffer), batch_size)\n",
        "        samples = random.choices(self.buffer, k=sample_size)\n",
        "        return map(list, zip(*samples))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rhZqiaDbSpj"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, env):\n",
        "        self.state_dim = env.observation_space.shape\n",
        "        self.action_size = env.action_space.n\n",
        "        self.q_network = QNetwork(self.state_dim, self.action_size)\n",
        "        self.replay_buffer = ReplayBuffer(maxlen=10000)\n",
        "        self.gamma = 0.97\n",
        "        self.eps = 1.0\n",
        "        \n",
        "        \n",
        "    def get_action(self, state):\n",
        "        q_state = self.q_network.get_q_state(self.sess, [state])\n",
        "        action_greedy = np.argmax(q_state)\n",
        "        action_random = np.random.randint(self.action_size)\n",
        "        action = action_random if random.random() < self.eps else action_greedy\n",
        "        return action\n",
        "    \n",
        "    def train(self, state, action, next_state, reward, done):\n",
        "        self.replay_buffer.add((state, action, next_state, reward, done))\n",
        "        states, actions, next_states, rewards, dones = self.replay_buffer.sample(50)\n",
        "        q_next_states = self.q_network.get_q_state(self.sess, next_states)\n",
        "        q_next_states[dones] = np.zeros([self.action_size])\n",
        "        q_targets = rewards + self.gamma * np.max(q_next_states, axis=1)\n",
        "        self.q_network.update_model(self.sess, states, actions, q_targets)\n",
        "        \n",
        "        if done: self.eps = max(0.1, 0.99*self.eps)\n",
        "    \n",
        "    def __del__(self):\n",
        "        self.sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5KmDntLAjuf"
      },
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size, replace,\n",
        "                 input_dims, eps_dec=0.996,  eps_min=0.01,\n",
        "                 mem_size=1000000, q_eval_fname='q_eval.h5',\n",
        "                 q_target_fname='q_next.h5'):\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_dec = eps_dec\n",
        "        self.eps_min = eps_min\n",
        "        self.batch_size = batch_size\n",
        "        self.replace = replace\n",
        "        self.q_target_model_file = q_target_fname\n",
        "        self.q_eval_model_file = q_eval_fname\n",
        "        self.learn_step = 0\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "        self.q_eval = build_dqn(alpha, n_actions, input_dims, 512)\n",
        "        self.q_next = build_dqn(alpha, n_actions, input_dims, 512)\n",
        "\n",
        "    def replace_target_network(self):\n",
        "        if self.replace is not None and self.learn_step % self.replace == 0:\n",
        "            self.q_next.set_weights(self.q_eval.get_weights())\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            state = np.array([observation], copy=False, dtype=np.float32)\n",
        "            actions = self.q_eval.predict(state)\n",
        "            action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr > self.batch_size:\n",
        "            state, action, reward, new_state, done = \\\n",
        "                                    self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "            self.replace_target_network()\n",
        "\n",
        "            q_eval = self.q_eval.predict(state)\n",
        "\n",
        "            q_next = self.q_next.predict(new_state)\n",
        "\n",
        "            \"\"\"\n",
        "            Thanks to Maximus-Kranic for pointing out this subtle bug.\n",
        "            q_next[done] = 0.0 works in Torch; it sets q_next to 0\n",
        "            for every index that done == 1. The behavior is different in\n",
        "            Keras, as you can verify by printing out q_next to the terminal\n",
        "            when done.any() == 1.\n",
        "            Despite this, the agent still manages to learn. Odd.\n",
        "            The correct implementation in Keras is to use q_next * (1-done)\n",
        "            q_next[done] = 0.0\n",
        "            q_target = q_eval[:]\n",
        "            indices = np.arange(self.batch_size)\n",
        "            q_target[indices, action] = reward + \\\n",
        "                                        self.gamma*np.max(q_next,axis=1)\n",
        "            \"\"\"\n",
        "            q_target = q_eval[:]\n",
        "            indices = np.arange(self.batch_size)\n",
        "            q_target[indices, action] = reward + \\\n",
        "                                    self.gamma*np.max(q_next, axis=1)*(1 - done)\n",
        "            self.q_eval.train_on_batch(state, q_target)\n",
        "\n",
        "            self.epsilon = self.epsilon - self.eps_dec \\\n",
        "                           if self.epsilon > self.eps_min else self.eps_min\n",
        "            self.learn_step += 1\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_eval.save(self.q_eval_model_file)\n",
        "        self.q_next.save(self.q_target_model_file)\n",
        "        print('... saving models ...')\n",
        "\n",
        "    def load_models(self):\n",
        "        self.q_eval = load_model(self.q_eval_model_file)\n",
        "        self.q_nexdt = load_model(self.q_target_model_file)\n",
        "        print('... loading models ...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5cREMkPGvff"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size, input_shape):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                      dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                          dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, terminal\n",
        "\n",
        "def build_dqn(lr, n_actions, input_dims, fc1_dims):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=32, kernel_size=8, strides=4, activation='relu',\n",
        "                     input_shape=(*input_dims,), data_format='channels_first'))\n",
        "    model.add(Conv2D(filters=64, kernel_size=4, strides=2, activation='relu',\n",
        "                     data_format='channels_first'))\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu',\n",
        "                     data_format='channels_first'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(fc1_dims, activation='relu'))\n",
        "    model.add(Dense(n_actions))\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=lr), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "class Agent(object):\n",
        "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size, replace,\n",
        "                 input_dims, eps_dec=0.996,  eps_min=0.01,\n",
        "                 mem_size=1000000, q_eval_fname='q_eval.h5',\n",
        "                 q_target_fname='q_next.h5'):\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_dec = eps_dec\n",
        "        self.eps_min = eps_min\n",
        "        self.batch_size = batch_size\n",
        "        self.replace = replace\n",
        "        self.q_target_model_file = q_target_fname\n",
        "        self.q_eval_model_file = q_eval_fname\n",
        "        self.learn_step = 0\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "        self.q_eval = build_dqn(alpha, n_actions, input_dims, 512)\n",
        "        self.q_next = build_dqn(alpha, n_actions, input_dims, 512)\n",
        "\n",
        "    def replace_target_network(self):\n",
        "        if self.replace is not None and self.learn_step % self.replace == 0:\n",
        "            self.q_next.set_weights(self.q_eval.get_weights())\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            state = np.array([observation], copy=False, dtype=np.float32)\n",
        "            actions = self.q_eval.predict(state)\n",
        "            action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr > self.batch_size:\n",
        "            state, action, reward, new_state, done = \\\n",
        "                                    self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "            self.replace_target_network()\n",
        "\n",
        "            q_eval = self.q_eval.predict(state)\n",
        "\n",
        "            q_next = self.q_next.predict(new_state)\n",
        "\n",
        "            \"\"\"\n",
        "            Thanks to Maximus-Kranic for pointing out this subtle bug.\n",
        "            q_next[done] = 0.0 works in Torch; it sets q_next to 0\n",
        "            for every index that done == 1. The behavior is different in\n",
        "            Keras, as you can verify by printing out q_next to the terminal\n",
        "            when done.any() == 1.\n",
        "            Despite this, the agent still manages to learn. Odd.\n",
        "            The correct implementation in Keras is to use q_next * (1-done)\n",
        "            q_next[done] = 0.0\n",
        "            q_target = q_eval[:]\n",
        "            indices = np.arange(self.batch_size)\n",
        "            q_target[indices, action] = reward + \\\n",
        "                                        self.gamma*np.max(q_next,axis=1)\n",
        "            \"\"\"\n",
        "            q_target = q_eval[:]\n",
        "            indices = np.arange(self.batch_size)\n",
        "            q_target[indices, action] = reward + \\\n",
        "                                    self.gamma*np.max(q_next, axis=1)*(1 - done)\n",
        "            self.q_eval.train_on_batch(state, q_target)\n",
        "\n",
        "            self.epsilon = self.epsilon - self.eps_dec \\\n",
        "                           if self.epsilon > self.eps_min else self.eps_min\n",
        "            self.learn_step += 1\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_eval.save(self.q_eval_model_file)\n",
        "        self.q_next.save(self.q_target_model_file)\n",
        "        print('... saving models ...')\n",
        "\n",
        "    def load_models(self):\n",
        "        self.q_eval = load_model(self.q_eval_model_file)\n",
        "        self.q_nexdt = load_model(self.q_target_model_file)\n",
        "        print('... loading models ...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWTgML3Gb8q5"
      },
      "source": [
        "def policy_fun(obs, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    An eps greedy policy\n",
        "    \"\"\"\n",
        "    if rng.random() < eps:\n",
        "        # eps\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        # Greedy\n",
        "        with torch.no_grad():\n",
        "            q_vals = net(obs)\n",
        "        assert q_vals.isfinite().all(), q_vals\n",
        "        a = int(q_vals.argmax())\n",
        "        return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nWtF6vlvG-rC",
        "outputId": "19567e1b-133e-45da-c5e2-c13d5b44f1a9"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.optimizers import Adam\n",
        "\n",
        "# One way of defining a model\n",
        "#p_inputSpace = 4\n",
        "#p_outputSpace = 2\n",
        "#alpha = 0.001\n",
        "#model=Sequential()\n",
        "#model.add(Dense(24,input_shape=(p_inputSpace,), activation=\"relu\"))\n",
        "#model.add(Dense(24, activation=\"relu\"))\n",
        "#model.add(Dense(p_outputSpace, activation=\"linear\"))\n",
        "#model.compile(loss=\"mse\", optimizer=Adam(lr=alpha))\n",
        "#Dense = Fully Connected\n",
        "\n",
        "# f : R^1 -> R^1\n",
        "p_inputSpace = 1\n",
        "p_outputSpace = 1\n",
        "\n",
        "# define the model\n",
        "inputs=keras.Input(shape=(p_inputSpace,),name=\"states\")\n",
        "x1 = keras.layers.Dense(64,activation=\"relu\")(inputs)\n",
        "x2 = keras.layers.Dense(64,activation=\"relu\")(x1)\n",
        "outputs = keras.layers.Dense(p_outputSpace, name=\"actions\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# plot it\n",
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    #to_file=\"model.png\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\", #TB: vertical; LR: hor\n",
        "    expand_nested=True,\n",
        "    dpi=96,\n",
        ")\n",
        "\n",
        "# optimizer and per-prediction error\n",
        "alpha = 0.001\n",
        "optimizer=tf.keras.optimizers.SGD(learning_rate=alpha)\n",
        "f_ppError = keras.losses.MeanSquaredError()\n",
        "\n",
        "# training data (plotted at the end of this file)\n",
        "train = np.array([[-2,-10], [-1,-5], [0, 0], [1,5], [2,10]])\n",
        "x_train = train[:,0]\n",
        "y_train = train[:,1]\n",
        "# for stochastic gradient descent, create batches\n",
        "batch_size = 1\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "epochs = 40\n",
        "for epoch in range(epochs):\n",
        "  #print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "  for step, (x_batch_train,y_batch_train) in enumerate(train_dataset):\n",
        "    #print(step,x_batch_train,y_batch_train)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(x_batch_train,training=True)\n",
        "      ppError = f_ppError(y_batch_train,predictions)\n",
        "    grads = tape.gradient(ppError,model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    print(\"Error: %f\" % float(ppError))\n",
        "\n",
        "\n",
        "\n",
        "D=np.arange(-2,2,.1)\n",
        "R=model.predict(D)\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(x_train,y_train,'gx',D,R,'r-')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 0.000000\n",
            "Error: 23.478329\n",
            "Error: 108.414383\n",
            "Error: 25.779203\n",
            "Error: 93.865990\n",
            "Error: 25.626131\n",
            "Error: 91.044991\n",
            "Error: 102.468880\n",
            "Error: 22.088985\n",
            "Error: 0.000001\n",
            "Error: 0.000001\n",
            "Error: 98.770470\n",
            "Error: 88.153084\n",
            "Error: 21.032459\n",
            "Error: 23.738142\n",
            "Error: 84.239929\n",
            "Error: 23.614971\n",
            "Error: 94.598503\n",
            "Error: 20.166710\n",
            "Error: 0.000094\n",
            "Error: 0.000093\n",
            "Error: 19.817223\n",
            "Error: 79.637161\n",
            "Error: 22.710495\n",
            "Error: 91.130623\n",
            "Error: 87.864182\n",
            "Error: 19.036726\n",
            "Error: 20.365404\n",
            "Error: 0.000533\n",
            "Error: 76.436981\n",
            "Error: 83.995842\n",
            "Error: 0.000372\n",
            "Error: 73.256348\n",
            "Error: 16.709461\n",
            "Error: 19.548719\n",
            "Error: 19.115829\n",
            "Error: 16.531231\n",
            "Error: 0.001099\n",
            "Error: 67.434669\n",
            "Error: 79.350327\n",
            "Error: 75.523453\n",
            "Error: 0.000133\n",
            "Error: 15.274153\n",
            "Error: 16.768902\n",
            "Error: 62.806633\n",
            "Error: 13.597558\n",
            "Error: 70.968948\n",
            "Error: 57.150562\n",
            "Error: 15.643748\n",
            "Error: 0.003768\n",
            "Error: 65.672050\n",
            "Error: 52.739140\n",
            "Error: 0.005178\n",
            "Error: 10.753822\n",
            "Error: 14.184308\n",
            "Error: 46.388874\n",
            "Error: 8.905000\n",
            "Error: 60.553253\n",
            "Error: 12.619615\n",
            "Error: 0.007180\n",
            "Error: 12.108761\n",
            "Error: 8.726495\n",
            "Error: 53.178143\n",
            "Error: 38.960518\n",
            "Error: 0.009914\n",
            "Error: 10.455976\n",
            "Error: 33.805508\n",
            "Error: 5.629431\n",
            "Error: 0.034436\n",
            "Error: 47.185825\n",
            "Error: 27.508114\n",
            "Error: 8.840500\n",
            "Error: 4.150278\n",
            "Error: 40.801006\n",
            "Error: 0.010589\n",
            "Error: 35.607315\n",
            "Error: 0.000154\n",
            "Error: 5.653961\n",
            "Error: 21.949265\n",
            "Error: 2.910302\n",
            "Error: 0.010586\n",
            "Error: 16.576136\n",
            "Error: 29.435770\n",
            "Error: 4.214125\n",
            "Error: 1.855836\n",
            "Error: 1.665262\n",
            "Error: 3.908941\n",
            "Error: 11.557054\n",
            "Error: 22.438242\n",
            "Error: 0.005685\n",
            "Error: 2.614336\n",
            "Error: 0.001712\n",
            "Error: 17.157310\n",
            "Error: 8.748829\n",
            "Error: 0.531586\n",
            "Error: 13.443639\n",
            "Error: 6.027062\n",
            "Error: 0.209244\n",
            "Error: 1.003489\n",
            "Error: 0.000299\n",
            "Error: 0.885604\n",
            "Error: 0.000043\n",
            "Error: 9.257741\n",
            "Error: 4.100538\n",
            "Error: 0.056614\n",
            "Error: 2.713399\n",
            "Error: 0.410183\n",
            "Error: 0.001988\n",
            "Error: 6.535848\n",
            "Error: 0.002975\n",
            "Error: 0.002708\n",
            "Error: 0.134036\n",
            "Error: 1.809321\n",
            "Error: 4.527618\n",
            "Error: 0.009371\n",
            "Error: 3.138600\n",
            "Error: 1.224656\n",
            "Error: 0.002154\n",
            "Error: 0.045177\n",
            "Error: 0.011650\n",
            "Error: 0.039291\n",
            "Error: 2.156311\n",
            "Error: 0.874827\n",
            "Error: 0.015721\n",
            "Error: 0.037468\n",
            "Error: 0.012652\n",
            "Error: 0.030099\n",
            "Error: 0.090182\n",
            "Error: 1.557571\n",
            "Error: 0.599239\n",
            "Error: 0.016039\n",
            "Error: 0.136226\n",
            "Error: 0.087531\n",
            "Error: 1.102416\n",
            "Error: 0.429631\n",
            "Error: 0.267671\n",
            "Error: 0.232175\n",
            "Error: 0.714993\n",
            "Error: 0.240094\n",
            "Error: 0.021693\n",
            "Error: 0.194230\n",
            "Error: 0.015330\n",
            "Error: 0.203229\n",
            "Error: 0.262745\n",
            "Error: 0.635800\n",
            "Error: 0.404564\n",
            "Error: 0.028167\n",
            "Error: 0.158876\n",
            "Error: 0.278841\n",
            "Error: 0.320717\n",
            "Error: 0.233888\n",
            "Error: 0.025197\n",
            "Error: 0.341254\n",
            "Error: 0.341826\n",
            "Error: 0.186900\n",
            "Error: 0.287627\n",
            "Error: 0.354290\n",
            "Error: 0.017868\n",
            "Error: 0.109419\n",
            "Error: 0.305430\n",
            "Error: 0.255649\n",
            "Error: 0.111229\n",
            "Error: 0.019739\n",
            "Error: 0.357967\n",
            "Error: 0.306895\n",
            "Error: 0.104510\n",
            "Error: 0.296805\n",
            "Error: 0.303572\n",
            "Error: 0.300625\n",
            "Error: 0.019837\n",
            "Error: 0.187803\n",
            "Error: 0.250652\n",
            "Error: 0.391108\n",
            "Error: 0.146774\n",
            "Error: 0.016790\n",
            "Error: 0.313854\n",
            "Error: 0.270254\n",
            "Error: 0.127869\n",
            "Error: 0.253438\n",
            "Error: 0.016210\n",
            "Error: 0.074805\n",
            "Error: 0.329773\n",
            "Error: 0.007803\n",
            "Error: 0.322774\n",
            "Error: 0.223545\n",
            "Error: 0.081037\n",
            "Error: 0.012591\n",
            "Error: 0.309479\n",
            "Error: 0.136836\n",
            "Error: 0.400926\n",
            "Error: 0.329533\n",
            "Error: 0.009515\n",
            "Error: 0.079964\n",
            "Error: 0.309048\n",
            "Error: 0.210525\n",
            "Error: 0.251952\n",
            "Error: 0.345183\n",
            "Error: 0.134392\n",
            "Error: 0.010085\n",
            "Error: 0.193097\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6113bc8090>,\n",
              " <matplotlib.lines.Line2D at 0x7f6113bdbb10>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}