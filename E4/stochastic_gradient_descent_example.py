# -*- coding: utf-8 -*-
"""stochastic_gradient_descent_example

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NNX1DdBoxq0sX_ill6yjCP-vq_QBhb15
"""

import numpy as np
import tensorflow as tf
import keras
#from keras.models import Sequential
#from keras.layers import Dense
#from keras.optimizers import Adam

# One way of defining a model
#p_inputSpace = 4
#p_outputSpace = 2
#alpha = 0.001
#model=Sequential()
#model.add(Dense(24,input_shape=(p_inputSpace,), activation="relu"))
#model.add(Dense(24, activation="relu"))
#model.add(Dense(p_outputSpace, activation="linear"))
#model.compile(loss="mse", optimizer=Adam(lr=alpha))
#Dense = Fully Connected

# f : R^1 -> R^1
p_inputSpace = 1
p_outputSpace = 1

# define the model
inputs=keras.Input(shape=(p_inputSpace,),name="states")
x1 = keras.layers.Dense(64,activation="relu")(inputs)
x2 = keras.layers.Dense(64,activation="relu")(x1)
outputs = keras.layers.Dense(p_outputSpace, name="actions")(x2)
model = keras.Model(inputs=inputs, outputs=outputs)

# plot it
keras.utils.plot_model(
    model,
    #to_file="model.png",
    show_shapes=True,
    show_dtype=True,
    show_layer_names=True,
    rankdir="TB", #TB: vertical; LR: hor
    expand_nested=True,
    dpi=96,
)

# optimizer and per-prediction error
alpha = 0.001
optimizer=keras.optimizers.SGD(learning_rate=alpha)
f_ppError = keras.losses.MeanSquaredError()

# training data (plotted at the end of this file)
train = np.array([[-2,-10], [-1,-5], [0, 0], [1,5], [2,10]])
x_train = train[:,0]
y_train = train[:,1]
# for stochastic gradient descent, create batches
batch_size = 1
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

epochs = 40
for epoch in range(epochs):
  #print("\nStart of epoch %d" % (epoch,))
  for step, (x_batch_train,y_batch_train) in enumerate(train_dataset):
    #print(step,x_batch_train,y_batch_train)
    with tf.GradientTape() as tape:
      predictions = model(x_batch_train,training=True)
      ppError = f_ppError(y_batch_train,predictions)
    grads = tape.gradient(ppError,model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    print("Error: %f" % float(ppError))



D=np.arange(-2,2,.1)
R=model.predict(D)
from matplotlib import pyplot as plt
plt.plot(x_train,y_train,'gx',D,R,'r-')

