{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HsGqubiDkQnd"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7iS40R9okStg"
   },
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.74\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVX1AjRWkueO",
    "outputId": "b797e8c0-7d2f-4cc3-d864-9bb82414a927"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 4, 0.0, False)],\n",
       "  2: [(1.0, 1, 0.0, False)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 1: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 5, 0.0, True)],\n",
       "  2: [(1.0, 2, 0.0, False)],\n",
       "  3: [(1.0, 1, 0.0, False)]},\n",
       " 2: {0: [(1.0, 1, 0.0, False)],\n",
       "  1: [(1.0, 6, 0.0, False)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 3: {0: [(1.0, 2, 0.0, False)],\n",
       "  1: [(1.0, 7, 0.0, True)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 3, 0.0, False)]},\n",
       " 4: {0: [(1.0, 4, 0.0, False)],\n",
       "  1: [(1.0, 8, 0.0, False)],\n",
       "  2: [(1.0, 5, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(1.0, 5, 0.0, True)],\n",
       "  1: [(1.0, 10, 0.0, False)],\n",
       "  2: [(1.0, 7, 0.0, True)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 12, 0.0, True)],\n",
       "  2: [(1.0, 9, 0.0, False)],\n",
       "  3: [(1.0, 4, 0.0, False)]},\n",
       " 9: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 10, 0.0, False)],\n",
       "  3: [(1.0, 5, 0.0, True)]},\n",
       " 10: {0: [(1.0, 9, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 11, 0.0, True)],\n",
       "  3: [(1.0, 6, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(1.0, 12, 0.0, True)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 14, 0.0, False)],\n",
       "  3: [(1.0, 9, 0.0, False)]},\n",
       " 14: {0: [(1.0, 13, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 15, 1.0, True)],\n",
       "  3: [(1.0, 10, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the gridworld-like environment\n",
    "env=gym.make('FrozenLakeNotSlippery-v0')\n",
    "# Let's look at the model of the environment (i.e., P):\n",
    "env.env.P\n",
    "# Question: what is the data in this structure saying? Relate this to the course\n",
    "# presentation of P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYKO1Ff7atFC"
   },
   "source": [
    "Data in this structure represents the dynamics function p. The first number states the current state index, the number from 0 to 3 represents the four actions the agent can take at each state(0 : $\\leftarrow$, 1 : $\\downarrow$, 2 : $\\rightarrow$, 3 : $\\uparrow$). The four elements in the list represent to:\n",
    "1. Probability of transitioning to the next state. Because the environment is deterministic, the probability equals 1 all the time.\n",
    "2. The index of the next state\n",
    "3. The reward. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "4. Whether the state ends the episode. It is `True` if the state is the hole or goal, `False` if the state is the starting point or frozen surface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gyn_w3ulkyZI",
    "outputId": "ea84c040-d973-4755-c188-26fb079b4d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# Now let's investigate the observation space (i.e., S using our nomenclature),\n",
    "# and confirm we see it is a discrete space with 16 locations\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zND5ArI8k_qQ",
    "outputId": "08338f8d-5d2d-41e8-a493-2c1296397399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "stateSpaceSize = env.observation_space.n\n",
    "print(stateSpaceSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_tp9YzRljnj",
    "outputId": "46f80a3d-02ba-4eba-f6e6-ce9b85147ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# Now let's investigate the action space (i.e., A) for the agent->environment\n",
    "# channel\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFGNZNowluz2",
    "outputId": "988358ee-15ed-4765-a3c6-b2fd6ac65063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample from S: 15  ...  sample from A: 0\n",
      "sample from S: 11  ...  sample from A: 2\n",
      "sample from S: 6  ...  sample from A: 1\n",
      "sample from S: 1  ...  sample from A: 1\n",
      "sample from S: 12  ...  sample from A: 0\n",
      "sample from S: 3  ...  sample from A: 1\n",
      "sample from S: 15  ...  sample from A: 0\n",
      "sample from S: 8  ...  sample from A: 2\n",
      "sample from S: 10  ...  sample from A: 1\n"
     ]
    }
   ],
   "source": [
    "# The gym environment has ...sample() functions that allow us to sample\n",
    "# from the above spaces:\n",
    "for g in range(1,10,1):\n",
    "  print(\"sample from S:\",env.observation_space.sample(),\" ... \",\"sample from A:\",env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOQL5JxsmcEd",
    "outputId": "c9885d17-4a98-439e-8b08-83b0b57c57fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# The enviroment also provides a helper to render (visualize) the environment\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLV6e43mmwx1",
    "outputId": "9db9cde1-964d-43fe-e9af-fa0bcfea1f90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Enter the action as an integer from 0 to 4  (or exit): \n",
      "1\n",
      "--> The result of taking action 1 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Enter the action as an integer from 0 to 4  (or exit): \n",
      "2\n",
      "--> The result of taking action 2 is:\n",
      "     S= 5\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Enter the action as an integer from 0 to 4  (or exit): \n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "# We can act as the agent, by selecting actions and stepping the environment\n",
    "# through time to see its responses to our actions\n",
    "env.reset()\n",
    "exitCommand=False\n",
    "while not(exitCommand):\n",
    "  env.render()\n",
    "  print(\"Enter the action as an integer from 0 to\",env.action_space.n,\" (or exit): \")\n",
    "  userInput=input()\n",
    "  if userInput==\"exit\":\n",
    "    break\n",
    "  action=int(userInput)\n",
    "  (observation, reward, compute, probability) = env.step(action)\n",
    "  print(\"--> The result of taking action\",action,\"is:\")\n",
    "  print(\"     S=\",observation)\n",
    "  print(\"     R=\",reward)\n",
    "  print(\"     p=\",probability)\n",
    "\n",
    "  env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tBpeiuRnyih"
   },
   "outputs": [],
   "source": [
    "# Question: draw a table indicating the correspondence between the action\n",
    "# you input (a number) and the logic action performed.\n",
    "# Question: draw a table that illustrates what the symbols on the render image\n",
    "# mean?\n",
    "# Question: Explain what the objective of the agent is in this environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C89riS7mUTiB"
   },
   "source": [
    "| Number | Action |\n",
    "| ----------- | ----------- |\n",
    "| `0` | $\\leftarrow$ left |\n",
    "| `1` | $\\downarrow$ down | \n",
    "| `2` | $\\rightarrow$ right| \n",
    "| `3` | $\\uparrow$ up|\n",
    "\n",
    "\n",
    "---\n",
    "| Symbol | Meaning |\n",
    "|---------|--------|\n",
    "| S | starting point, safe |\n",
    "| F | frozen surface, safe |\n",
    "| H | hole, fall to your doom |\n",
    "| G | goal, where the frisbee is located|\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The object of the agent is to reach the goal(G).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWI3h6s7qqdq"
   },
   "outputs": [],
   "source": [
    "# Practical: Code up an AI that will employ random action selection in order\n",
    "# to drive the agent. Test this random action selection agent with the\n",
    "# above environment (i.e., code up a loop as I did above, but instead\n",
    "# of taking input from a human user, take it from the AI you coded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmRwGwPoqw0F",
    "outputId": "c62e65f0-81a7-4597-8bca-dd9698cb1740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 3 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 3 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 5\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "#An AI that will employ random action selection\n",
    "def random_ai(env,policy):\n",
    "  env.reset()\n",
    "  exitCommand = False\n",
    "  env.render()\n",
    "  while not(exitCommand):\n",
    "    action = policy(env)\n",
    "    (observation, reward, compute, probability) = env.step(action)\n",
    "    print(\"--> The result of taking action\",action,\"is:\")\n",
    "    print(\"     S=\",observation)\n",
    "    print(\"     R=\",reward)\n",
    "    print(\"     p=\",probability)\n",
    "\n",
    "    env.render()\n",
    "    #exit when the agent terminates the episode\n",
    "    if compute is True:\n",
    "      exitCommand = True\n",
    "\n",
    "env.seed(3)\n",
    "policy = lambda env: env.action_space.sample()\n",
    "random_ai(env,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQLKuubzrl4L"
   },
   "outputs": [],
   "source": [
    "# Now towards dynamic programming. Note that env.env.P has the model\n",
    "# of the environment.\n",
    "#\n",
    "# Question: How would you represent the agent's policy function and value function?\n",
    "# Practical: revise the above AI solver to use a policy function in which you\n",
    "# code the random action selections in the policy function. Test this.\n",
    "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\n",
    "# either the inplace or ping-pong buffer (as described in the lecture). Now\n",
    "# randomly initialize your policy function, and compute its value function.\n",
    "# Report your results: policy and value function. Ensure your prediction\n",
    "# algo reports how many iterations it took.\n",
    "#\n",
    "# (Optional): Repeat the above for q.\n",
    "#\n",
    "# Policy Improvement:\n",
    "# Question: How would you use P and your value function to improve an arbitrary\n",
    "# policy, pi, per Chapter 4?\n",
    "# Practical: Code the policy iteration process, and employ it to arrive at a\n",
    "# policy that solves this problem. Show your testing results, and ensure\n",
    "# it reports the number of iterations for each step: (a) overall policy\n",
    "# iteration steps and (b) evaluation steps.\n",
    "# Practical: Code the value iteration process, and employ it to arrive at a\n",
    "# policy that solves this problem. Show your testing results, reporting\n",
    "# the iteration counts.\n",
    "# Comment on the difference between the iterations required for policy vs\n",
    "# value iteration.\n",
    "#\n",
    "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\n",
    "# env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-TIIQZ-5BDQ"
   },
   "source": [
    "\n",
    "\n",
    "> Question: How would you represent the agent's policy function and value function?\n",
    "\n",
    "The agent's policy function can be implemented as an N-element(N is the length of the observation_space, the number of states) array lookup table of integer(0 to 3), representing choosing the action at that state.\n",
    "\n",
    "The value function can be implemented as an N-element array lookup table of float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIMXApox916y",
    "outputId": "b039f1fb-c31c-46a8-fa3c-5c39dd13290c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 9\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 13\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 14\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 15\n",
      "     R= 1.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Practical: revise the above AI solver to use a policy function in which you\n",
    "# code the random action selections in the policy function. Test this.\n",
    "manual_policy = [1,2,1,0,\n",
    "                 1,1,1,0,\n",
    "                 2,1,1,0,\n",
    "                 2,2,2,1]\n",
    "policy = lambda env: manual_policy[env.env.s]\n",
    "random_ai(env,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crpc2EiTB4zm",
    "outputId": "4d58315e-a83a-4b8b-849a-b52b8af74943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random policy:\n",
      "[[0 3 1 0]\n",
      " [3 3 3 3]\n",
      " [1 3 1 2]\n",
      " [0 3 2 0]]\n",
      "policy prediction converged in 3 itreations\n",
      "Final values:\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.9 0. ]\n",
      " [0.  0.  1.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\n",
    "# either the inplace or ping-pong buffer (as described in the lecture). Now\n",
    "# randomly initialize your policy function, and compute its value function.\n",
    "# Report your results: policy and value function. Ensure your prediction\n",
    "# algo reports how many iterations it took.\n",
    "def policy_prediction(policy, threshold, gamma, V, env):\n",
    "  # Initialize V(s), for all s in S+, arbitrarily except that V(terminal) = 0\n",
    "  S = env.observation_space.n\n",
    "  if V is None:\n",
    "    V = np.zeros(S)\n",
    "  \n",
    "  iterations = 0\n",
    "  converged = False\n",
    "  while not converged:\n",
    "    delta = 0\n",
    "    iterations += 1\n",
    "\n",
    "    for state in range(S):\n",
    "      v = V[state]\n",
    "      #Assumes the policy only returns one action with probability 1.\n",
    "      action = policy[state]\n",
    "      sum = 0\n",
    "      for p, next_state, r, terminate in env.env.P[state][action]: \n",
    "        sum += p*(r + gamma*V[next_state])\n",
    "      V[state] = sum\n",
    "      delta = max(delta, np.abs(v - V[state]))\n",
    "    converged = True if delta < threshold else False\n",
    "  return V, iterations\n",
    "\n",
    "#randomly initialize your policy function\n",
    "print(\"Random policy:\")\n",
    "policy = np.random.RandomState(0).randint(0, 4, size=16)\n",
    "print(policy.reshape(4, 4))\n",
    "V, iterations = policy_prediction(policy, 1e-6, 0.9, None, env)\n",
    "print(\"policy prediction converged in %d itreations\"%iterations)\n",
    "print(\"Final values:\")\n",
    "print(V.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WUe0UAKBxV_"
   },
   "source": [
    "> Question: How would you use P and your value function to improve an arbitrary policy, pi, per Chapter 4?\n",
    "\n",
    "By using policy iteration which iteratively improves policies and value functions through implementing policy evaluation and policy improvement until we find the optimal policy and value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7OSm_K8gkTaD",
    "outputId": "0bbb633f-e2ab-433f-cd2a-776aaa2d0cf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random policy:\n",
      "[[1 2 3 1]\n",
      " [2 3 1 3]\n",
      " [2 2 3 2]\n",
      " [0 1 2 0]]\n",
      "policy iteration 1\n",
      "policy evaluation converged in 2 itreations\n",
      "policy iteration 2\n",
      "policy evaluation converged in 2 itreations\n",
      "policy iteration 3\n",
      "policy evaluation converged in 2 itreations\n",
      "policy iteration 4\n",
      "policy evaluation converged in 2 itreations\n",
      "policy iteration 5\n",
      "policy evaluation converged in 2 itreations\n",
      "policy iteration 6\n",
      "policy evaluation converged in 2 itreations\n",
      "Final value:\n",
      "[[0.59049 0.6561  0.729   0.6561 ]\n",
      " [0.6561  0.      0.81    0.     ]\n",
      " [0.729   0.81    0.9     0.     ]\n",
      " [0.      0.9     1.      0.     ]]\n",
      "Final policy:\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# Practical: Code the policy iteration process, and employ it to arrive at a\n",
    "# policy that solves this problem. Show your testing results, and ensure\n",
    "# it reports the number of iterations for each step: (a) overall policy\n",
    "# iteration steps and (b) evaluation steps.\n",
    "def policy_iteration(V, gamma, threshold,env):\n",
    "  S = env.observation_space.n\n",
    "  A = env.action_space.n\n",
    "  #initialize\n",
    "  if V is None:\n",
    "    V = np.zeros(S)\n",
    "  pi = np.random.randint(0, A, size = S)\n",
    "  print(\"random policy:\")\n",
    "  print(pi.reshape(4,4))\n",
    "  policy_step = 0\n",
    "  while True:\n",
    "    policy_step += 1\n",
    "    print('policy iteration',policy_step)\n",
    "    #policy evaluation\n",
    "    V, iter = policy_prediction(pi,threshold,gamma,V,env)\n",
    "    print(\"policy evaluation converged in %d itreations\"%iter)\n",
    "    policy_stable = True\n",
    "    for state in range(S):\n",
    "      old_action = pi[state]\n",
    "      values = np.zeros(A)\n",
    "      for action in range(A):\n",
    "        for p, next_state, r, terminate in env.env.P[state][action]: \n",
    "          values[action] += p*(r+gamma*V[next_state])\n",
    "      pi[state] = np.argmax(values)\n",
    "      if old_action != pi[state]:\n",
    "        policy_stable = False\n",
    "    if policy_stable:\n",
    "      return V, pi\n",
    "\n",
    "V_star, pi_star = policy_iteration(None, 0.9, 1e-6, env)\n",
    "print(\"Final value:\")\n",
    "print(V_star.reshape(4, 4))\n",
    "print(\"Final policy:\")\n",
    "print(pi_star.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KyFVFX10bsk",
    "outputId": "96c8ac0a-f3ab-41a8-a9e8-7c884290a30d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 9\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 13\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 14\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 15\n",
      "     R= 1.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#test results\n",
    "policy = lambda env: pi_star[env.env.s]\n",
    "random_ai(env,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBxHU0vn1JEk",
    "outputId": "355b53df-7e36-45eb-a7c8-434526b840be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random policy:\n",
      "[[2 0 0 2]\n",
      " [2 2 2 3]\n",
      " [3 1 0 1]\n",
      " [0 0 2 0]]\n",
      "value interation converged in 7 itreations\n",
      "Final value:\n",
      "[[0.59049 0.6561  0.729   0.6561 ]\n",
      " [0.6561  0.      0.81    0.     ]\n",
      " [0.729   0.81    0.9     0.     ]\n",
      " [0.      0.9     1.      0.     ]]\n",
      "Final policy:\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# Practical: Code the value iteration process, and employ it to arrive at a\n",
    "# policy that solves this problem. Show your testing results, reporting\n",
    "# the iteration counts.\n",
    "def value_iteration(V, gamma, threshold,env):\n",
    "  S = env.observation_space.n\n",
    "  A = env.action_space.n\n",
    "  #initialize\n",
    "  if V is None:\n",
    "    V = np.zeros(S)\n",
    "  pi = np.random.randint(0, A, size = S)\n",
    "  print(\"random policy:\")\n",
    "  print(pi.reshape(4,4))\n",
    "  value_step = 0\n",
    "  while True:\n",
    "    delta = 0\n",
    "    value_step += 1\n",
    "    for state in range(S):\n",
    "      v = V[state]\n",
    "      values = np.zeros(A)\n",
    "      for action in range(A):\n",
    "        for p, next_state, r, terminate in env.env.P[state][action]: \n",
    "          values[action] += p*(r+gamma*V[next_state])\n",
    "      V[state] = max(values)\n",
    "      delta = max(delta,np.abs(v-V[state]))\n",
    "    if delta < threshold:\n",
    "      print(\"value interation converged in %d itreations\"%value_step)\n",
    "      break\n",
    "  pi = np.zeros(S,dtype=int)\n",
    "  for s in range(S):\n",
    "    values = np.zeros(A)\n",
    "    for action in range(A):\n",
    "      for p, next_state, r, terminate in env.env.P[s][action]: \n",
    "        values[action] += p*(r+gamma*V[next_state])\n",
    "    pi[s] = np.argmax(values)\n",
    "\n",
    "  return V, pi\n",
    "\n",
    "\n",
    "\n",
    "V_star, pi_star = value_iteration(None, 0.9, 1e-6, env)\n",
    "print(\"Final value:\")\n",
    "print(V_star.reshape(4, 4))\n",
    "print(\"Final policy:\")\n",
    "print(pi_star.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIWo_HBt7RjR",
    "outputId": "e9837e08-622f-4888-d7d0-01aff5986167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 9\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 13\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 14\n",
      "     R= 0.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 15\n",
      "     R= 1.0\n",
      "     p= {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#test results\n",
    "policy = lambda env: pi_star[env.env.s]\n",
    "random_ai(env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpYGRKck7dan"
   },
   "source": [
    "\n",
    "\n",
    "> Comment on the difference between the iterations required for policy vs value iteration.\n",
    "\n",
    "Policy iteration requires 6 iterations and each iteration takes 2 steps of policy evaluation. Value iteration requires 7 iterations. Value iteration converges faster than policy iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-35fsXlL9feV",
    "outputId": "cd6d7345-a3d5-4cd7-9dc2-39130064c4d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random policy:\n",
      "[[3 0 1 3]\n",
      " [0 2 0 1]\n",
      " [2 0 0 3]\n",
      " [3 1 1 3]]\n",
      "policy iteration 1\n",
      "policy evaluation converged in 45 itreations\n",
      "policy iteration 2\n",
      "policy evaluation converged in 80 itreations\n",
      "policy iteration 3\n",
      "policy evaluation converged in 13 itreations\n",
      "policy iteration 4\n",
      "policy evaluation converged in 48 itreations\n",
      "Final value:\n",
      "[[0.06889086 0.06141454 0.07440974 0.0558073 ]\n",
      " [0.09185451 0.         0.1122082  0.        ]\n",
      " [0.14543633 0.24749694 0.29961759 0.        ]\n",
      " [0.         0.37993589 0.63902014 0.        ]]\n",
      "Final policy:\n",
      "[[0 3 0 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\n",
    "# env = gym.make(\"FrozenLake-v0\")\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "#policy iteration\n",
    "V_star, pi_star = policy_iteration(None, 0.9, 1e-8, env)\n",
    "print(\"Final value:\")\n",
    "print(V_star.reshape(4, 4))\n",
    "print(\"Final policy:\")\n",
    "print(pi_star.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoS1fACl-Qcw",
    "outputId": "902fa359-f52a-4b13-dc93-515535a79907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 3 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 3 is:\n",
      "     S= 9\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 10\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 9\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 10\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 6\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 10\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 6\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 10\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 6\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 10\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 9\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 3 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 3 is:\n",
      "     S= 9\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 3 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 3 is:\n",
      "     S= 9\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 13\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 14\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 15\n",
      "     R= 1.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#test results\n",
    "policy = lambda env: pi_star[env.env.s]\n",
    "random_ai(env,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4mSMnoi_VK1",
    "outputId": "4a59fdaf-31d9-42d7-bd9a-9dd5c5ce0e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random policy:\n",
      "[[1 0 0 2]\n",
      " [0 1 0 2]\n",
      " [2 2 3 1]\n",
      " [1 0 1 1]]\n",
      "value interation converged in 85 itreations\n",
      "Final value:\n",
      "[[0.06889086 0.06141454 0.07440974 0.0558073 ]\n",
      " [0.09185451 0.         0.1122082  0.        ]\n",
      " [0.14543633 0.24749694 0.29961758 0.        ]\n",
      " [0.         0.37993589 0.63902014 0.        ]]\n",
      "Final policy:\n",
      "[[0 3 0 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#value iteration\n",
    "V_star, pi_star = value_iteration(None, 0.9, 1e-8, env)\n",
    "print(\"Final value:\")\n",
    "print(V_star.reshape(4, 4))\n",
    "print(\"Final policy:\")\n",
    "print(pi_star.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8jQKZLT_dlJ",
    "outputId": "659f90fb-8169-47ff-ac5d-72c8dbfc2005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 0\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 4\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--> The result of taking action 0 is:\n",
      "     S= 8\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "--> The result of taking action 3 is:\n",
      "     S= 9\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 13\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "--> The result of taking action 2 is:\n",
      "     S= 14\n",
      "     R= 0.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "--> The result of taking action 1 is:\n",
      "     S= 15\n",
      "     R= 1.0\n",
      "     p= {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#test results\n",
    "policy = lambda env: pi_star[env.env.s]\n",
    "random_ai(env,policy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
