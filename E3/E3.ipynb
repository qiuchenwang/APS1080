{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyqLQ6MD0wat"
      },
      "source": [
        "# Exercise III: Cart-Pole with TD(0)\n",
        ">Solve the Cart-Pole control problem using TD(0) methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf5Okpm_19jM"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOFYb0yt131z",
        "outputId": "3f1a3f4f-ba76-4b5f-c57a-5f29fa3a9122"
      },
      "source": [
        "#Cart-Pole Problem\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "    \n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step i 0 action= 0\n",
            "obs= [-0.03592544 -0.21272867  0.01448271  0.27566908] reward= 1.0 done= False info= {}\n",
            "step i 1 action= 0\n",
            "obs= [-0.04018002 -0.40805423  0.01999609  0.57288446] reward= 1.0 done= False info= {}\n",
            "step i 2 action= 0\n",
            "obs= [-0.0483411  -0.60345076  0.03145378  0.87179911] reward= 1.0 done= False info= {}\n",
            "step i 3 action= 0\n",
            "obs= [-0.06041012 -0.79898604  0.04888976  1.17420284] reward= 1.0 done= False info= {}\n",
            "step i 4 action= 0\n",
            "obs= [-0.07638984 -0.99470808  0.07237382  1.48180298] reward= 1.0 done= False info= {}\n",
            "step i 5 action= 1\n",
            "obs= [-0.096284   -0.80053984  0.10200988  1.2125724 ] reward= 1.0 done= False info= {}\n",
            "step i 6 action= 0\n",
            "obs= [-0.1122948  -0.99681963  0.12626132  1.53540046] reward= 1.0 done= False info= {}\n",
            "step i 7 action= 1\n",
            "obs= [-0.13223119 -0.80342403  0.15696933  1.2846386 ] reward= 1.0 done= False info= {}\n",
            "step i 8 action= 0\n",
            "obs= [-0.14829967 -1.00015689  0.18266211  1.62206951] reward= 1.0 done= False info= {}\n",
            "step i 9 action= 1\n",
            "obs= [-0.16830281 -0.80759532  0.2151035   1.39143698] reward= 1.0 done= True info= {}\n",
            "Iterations that were run: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRu3ih2a0042"
      },
      "source": [
        "## a. on-policy SARSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvlLHmtT0vJR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t49pT1jY1KEV"
      },
      "source": [
        "## b. off-policy Q learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdXSvKCg1PXe"
      },
      "source": [
        "## c. off-policy Expected SARSA with an epsilon-greedy policy"
      ]
    }
  ]
}