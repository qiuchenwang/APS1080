{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOHLFkcdoUDm"
      },
      "source": [
        "# A2: Monte Carlo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtzvN4e9onbh"
      },
      "source": [
        "## Monte Carlo Part A\n",
        ">1. Explain clearly why V_pi is not useful in the MC development above?\n",
        "\n",
        "Because state values V_pi alone simply leads us to the best combination of reward and next state S' following the policy pi. However, without the the transition dynamics p, we don't know what action can take us to state S'. \n",
        "\n",
        ">2. The MC algorithm so far (ref: p 99), requires an infinite number of episodes for Eval to converge on Q_pi_k (step k). We can modify this algorithm to the practical variant where Eval is truncated (c.f., DynProg GPI). In this case: \n",
        ">* a. Will we obtain Q_pi_k from eval?\n",
        "\n",
        "No. Since evaluation is truncated, we can only obtain an approximate value.\n",
        ">* b. If not why are we able to truncate Eval? Explain clearly.\n",
        "\n",
        "Because by implementing truncated evaluation and improvement repeatedly, the value function is altered to more closely approximate to true value function for the current policy, and the policy is improved concerning the current value function. These two kinds of changes together cause both the value function and the policy to converge to the optimality.\n",
        "\n",
        "\n",
        ">* c. Assuming ES (i.e., thorough sampling of the S x A space), and the above truncated Eval_trunc, is it possible to converge on a sub-optimal policy pi_c? Is this a stable fixed point of the GPI for MC? Explain clearly.\n",
        "\n",
        "It cannot converge to any sub-optimal policy. Because if it converges on a sub-optimal policy pi_c, then throughout policy evaluation repeatedly, the value function will converge to the real value function under policy pi_c. Then policy improvement will guarantee a new policy better than policy pi_c. So it will eventually lead to the optimal policy.\n",
        ">3. Explain how you can synthesize a stochastic policy given what you know so far (you don't need to read ahead).\n",
        "\n",
        "Start with a policy with non-zero values for each state-action pair so that all state-action pairs will be visited in a finite number of episodes.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEWmnr0Yq4g5"
      },
      "source": [
        "##Monte Carlo Part B-1: Stochastic Policies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ1jaXzqrEbt"
      },
      "source": [
        "##Monte Carlo Part B-2: Off Policy Methods\n",
        ">Code the algorithm for MC Control (Off Policy) and apply this to the Cart Pole problem. You must discretize the environmental feedback (S) in order to solve this problem properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFIKbYsZoPT9"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meKtIgZVD1cr",
        "outputId": "3dbd13c9-f9c9-4b69-b6b2-03c3f55a7a7a"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "    \n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step i 0 action= 0\n",
            "obs= [ 0.03098964 -0.21151883  0.03340614  0.27969223] reward= 1.0 done= False info= {}\n",
            "step i 1 action= 0\n",
            "obs= [ 0.02675926 -0.40710101  0.03899998  0.58272139] reward= 1.0 done= False info= {}\n",
            "step i 2 action= 0\n",
            "obs= [ 0.01861724 -0.60274703  0.05065441  0.88743035] reward= 1.0 done= False info= {}\n",
            "step i 3 action= 1\n",
            "obs= [ 0.0065623  -0.40834786  0.06840302  0.61109165] reward= 1.0 done= False info= {}\n",
            "step i 4 action= 0\n",
            "obs= [-0.00160466 -0.60435585  0.08062485  0.92451087] reward= 1.0 done= False info= {}\n",
            "step i 5 action= 0\n",
            "obs= [-0.01369178 -0.80046884  0.09911507  1.2414028 ] reward= 1.0 done= False info= {}\n",
            "step i 6 action= 1\n",
            "obs= [-0.02970115 -0.60674907  0.12394312  0.98134142] reward= 1.0 done= False info= {}\n",
            "step i 7 action= 1\n",
            "obs= [-0.04183613 -0.41348647  0.14356995  0.73001739] reward= 1.0 done= False info= {}\n",
            "step i 8 action= 1\n",
            "obs= [-0.05010586 -0.22060986  0.1581703   0.48574379] reward= 1.0 done= False info= {}\n",
            "step i 9 action= 1\n",
            "obs= [-0.05451806 -0.028032    0.16788518  0.2467913 ] reward= 1.0 done= False info= {}\n",
            "step i 10 action= 1\n",
            "obs= [-0.0550787   0.16434412  0.172821    0.01141094] reward= 1.0 done= False info= {}\n",
            "step i 11 action= 1\n",
            "obs= [-0.05179182  0.35662061  0.17304922 -0.22215062] reward= 1.0 done= False info= {}\n",
            "step i 12 action= 0\n",
            "obs= [-0.04465941  0.15950204  0.16860621  0.119734  ] reward= 1.0 done= False info= {}\n",
            "step i 13 action= 1\n",
            "obs= [-0.04146936  0.35185751  0.17100089 -0.11537199] reward= 1.0 done= False info= {}\n",
            "step i 14 action= 0\n",
            "obs= [-0.03443221  0.15475053  0.16869345  0.22600586] reward= 1.0 done= False info= {}\n",
            "step i 15 action= 0\n",
            "obs= [-0.0313372  -0.04233023  0.17321357  0.56679163] reward= 1.0 done= False info= {}\n",
            "step i 16 action= 1\n",
            "obs= [-0.03218381  0.14999298  0.1845494   0.33329422] reward= 1.0 done= False info= {}\n",
            "step i 17 action= 1\n",
            "obs= [-0.02918395  0.34207452  0.19121528  0.10401456] reward= 1.0 done= False info= {}\n",
            "step i 18 action= 1\n",
            "obs= [-0.02234246  0.53401525  0.19329557 -0.12277372] reward= 1.0 done= False info= {}\n",
            "step i 19 action= 0\n",
            "obs= [-0.01166215  0.3367252   0.1908401   0.22412567] reward= 1.0 done= False info= {}\n",
            "step i 20 action= 1\n",
            "obs= [-0.00492765  0.52868014  0.19532261 -0.00281234] reward= 1.0 done= False info= {}\n",
            "step i 21 action= 1\n",
            "obs= [ 0.00564595  0.72054277  0.19526637 -0.22807351] reward= 1.0 done= False info= {}\n",
            "step i 22 action= 1\n",
            "obs= [ 0.02005681  0.91241626  0.1907049  -0.45337002] reward= 1.0 done= False info= {}\n",
            "step i 23 action= 0\n",
            "obs= [ 0.03830513  0.71518169  0.1816375  -0.10715369] reward= 1.0 done= False info= {}\n",
            "step i 24 action= 0\n",
            "obs= [0.05260877 0.51798457 0.17949442 0.23688419] reward= 1.0 done= False info= {}\n",
            "step i 25 action= 1\n",
            "obs= [0.06296846 0.71014885 0.18423211 0.00575716] reward= 1.0 done= False info= {}\n",
            "step i 26 action= 0\n",
            "obs= [0.07717144 0.51292841 0.18434725 0.3504399 ] reward= 1.0 done= False info= {}\n",
            "step i 27 action= 1\n",
            "obs= [0.08743    0.70501575 0.19135605 0.12108259] reward= 1.0 done= False info= {}\n",
            "step i 28 action= 0\n",
            "obs= [0.10153032 0.50774083 0.1937777  0.46750971] reward= 1.0 done= False info= {}\n",
            "step i 29 action= 0\n",
            "obs= [0.11168514 0.31048515 0.20312789 0.81447018] reward= 1.0 done= False info= {}\n",
            "step i 30 action= 0\n",
            "obs= [0.11789484 0.11324742 0.2194173  1.16355384] reward= 1.0 done= True info= {}\n",
            "Iterations that were run: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwgUTJ_4Ekey"
      },
      "source": [
        "#discretize the environmental feedback (S)\n",
        "'''\n",
        "  Type: Box(4)\n",
        "  Num     Observation               Min                     Max\n",
        "  0       Cart Position             -4.8                    4.8\n",
        "  1       Cart Velocity             -Inf                    Inf\n",
        "  2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
        "  3       Pole Angular Velocity     -Inf                    Inf\n",
        "'''\n",
        "\n",
        "def discretize_state(obs):\n",
        "\t# env.observation_space.high\n",
        "\t# [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
        "\t# env.observation_space.low\n",
        "\t# [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
        "  discrete = [np.digitize(obs[i], bins) for i, bins in enumerate([\n",
        "    np.linspace(-4.8, 4.8, 9),\n",
        "    np.linspace(-4, 4, 7),\n",
        "    np.linspace(-0.418, 0.418, 9),\n",
        "    np.linspace(-4, 4, 7),\n",
        "    ])]\n",
        "  return ((obs > 0) * 8 **np.arange(len(obs))).sum()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtCw0IOHENAU"
      },
      "source": [
        "#policy initialization\n",
        "def init_policy(epsilon,S,A):\n",
        "  pi_init = np.random.random([S,A])\n",
        "  out = np.zeros_like(pi_init, dtype = np.float)\n",
        "  idx = pi_init.argmax(axis=1)\n",
        "  out[np.arange(S), idx] = 1\n",
        "  pi = out*(1 - epsilon) + epsilon / A\n",
        "  return pi"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBdqdtqxPCLb"
      },
      "source": [
        "#generate episode using b\n",
        "def generate_episode(env, policy, actions):\n",
        "  episode = []\n",
        "  obs = env.reset()\n",
        "  i = 0\n",
        "  while True:\n",
        "    i += 1\n",
        "    state = discretize_state(obs)\n",
        "    action = np.random.choice(actions, p=policy[state])\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    episode.append((state, action, reward))\n",
        "    if done:\n",
        "      break\n",
        "  return episode,i\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XjIwb9REEhX"
      },
      "source": [
        "#MC Control (Off Policy)\n",
        "def Monte_Carlo_off_policy(env,gamma,epsilon):\n",
        "  S = 8**len(obs)\n",
        "  A = env.action_space.n\n",
        "  #initialize\n",
        "  states = np.arange(S)\n",
        "  actions = np.arange(A)\n",
        "  Q = np.random.random([S,A])\n",
        "  C = np.zeros([S,A])\n",
        "  #initialize pi\n",
        "  pi = np.argmax(Q, axis = 1)\n",
        "\n",
        "\n",
        "  for episode in range(1000):\n",
        "    b = init_policy(epsilon,S,A)\n",
        "    #generate episodes\n",
        "    episode, i = generate_episode(env,b,actions)\n",
        "    #state_actions = [(s, a) for (s,a,r) in episode]\n",
        "    \n",
        "    G = 0\n",
        "    W = 1\n",
        "    for t in range(i-1,-1,-1):\n",
        "      state, action, reward = episode[t]\n",
        "      G = gamma * G + reward\n",
        "      C[state,action] += W\n",
        "      Q[state,action] += W/C[state,action]*(G-Q[state,action])\n",
        "      pi[state] = np.argmax(Q[state]) \n",
        "      if action != pi[state]:\n",
        "        break\n",
        "      W = W / b[state,action]\n",
        "\n",
        "  return Q, pi"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3J0soooNdce"
      },
      "source": [
        "Q,pi = Monte_Carlo_off_policy(env,0.99,0.01)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uuq8Nf2APqe0",
        "outputId": "4feb7d12-64c9-4842-84b2-1eb3245373fc"
      },
      "source": [
        "A = env.action_space.n\n",
        "# Test\n",
        "done = False\n",
        "obs = env.reset()\n",
        "for i in range(50000):\n",
        "    state = discretize_state(obs)\n",
        "    action = pi[state]\n",
        "\n",
        "    print(\"step i\",i,\"action=\",action)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step i 0 action= 1\n",
            "obs= [-0.0293961   0.23138337 -0.02315883 -0.26607751] reward= 1.0 done= False info= {}\n",
            "step i 1 action= 0\n",
            "obs= [-0.02476843  0.03659948 -0.02848038  0.0192119 ] reward= 1.0 done= False info= {}\n",
            "step i 2 action= 1\n",
            "obs= [-0.02403644  0.23211805 -0.02809614 -0.28231912] reward= 1.0 done= False info= {}\n",
            "step i 3 action= 0\n",
            "obs= [-0.01939408  0.03740788 -0.03374252  0.00137168] reward= 1.0 done= False info= {}\n",
            "step i 4 action= 1\n",
            "obs= [-0.01864592  0.23299709 -0.03371509 -0.30176356] reward= 1.0 done= False info= {}\n",
            "step i 5 action= 0\n",
            "obs= [-0.01398598  0.03837149 -0.03975036 -0.01990141] reward= 1.0 done= False info= {}\n",
            "step i 6 action= 0\n",
            "obs= [-0.01321855 -0.15615852 -0.04014839  0.25997957] reward= 1.0 done= False info= {}\n",
            "step i 7 action= 1\n",
            "obs= [-0.01634172  0.03951288 -0.0349488  -0.04509147] reward= 1.0 done= False info= {}\n",
            "step i 8 action= 0\n",
            "obs= [-0.01555147 -0.15509094 -0.03585063  0.23636316] reward= 1.0 done= False info= {}\n",
            "step i 9 action= 1\n",
            "obs= [-0.01865328  0.04052438 -0.03112336 -0.0674091 ] reward= 1.0 done= False info= {}\n",
            "step i 10 action= 0\n",
            "obs= [-0.0178428  -0.15413785 -0.03247154  0.21529404] reward= 1.0 done= False info= {}\n",
            "step i 11 action= 1\n",
            "obs= [-0.02092555  0.04143291 -0.02816566 -0.08745241] reward= 1.0 done= False info= {}\n",
            "step i 12 action= 0\n",
            "obs= [-0.0200969  -0.15327423 -0.02991471  0.19621285] reward= 1.0 done= False info= {}\n",
            "step i 13 action= 1\n",
            "obs= [-0.02316238  0.04226256 -0.02599046 -0.10575473] reward= 1.0 done= False info= {}\n",
            "step i 14 action= 0\n",
            "obs= [-0.02231713 -0.15247748 -0.02810555  0.17861634] reward= 1.0 done= False info= {}\n",
            "step i 15 action= 1\n",
            "obs= [-0.02536668  0.04303517 -0.02453322 -0.12279874] reward= 1.0 done= False info= {}\n",
            "step i 16 action= 0\n",
            "obs= [-0.02450598 -0.15172687 -0.0269892   0.16204435] reward= 1.0 done= False info= {}\n",
            "step i 17 action= 1\n",
            "obs= [-0.02754051  0.04377085 -0.02374831 -0.13902929] reward= 1.0 done= False info= {}\n",
            "step i 18 action= 0\n",
            "obs= [-0.0266651  -0.15100306 -0.0265289   0.14606784] reward= 1.0 done= False info= {}\n",
            "step i 19 action= 1\n",
            "obs= [-0.02968516  0.04448856 -0.02360754 -0.15486499] reward= 1.0 done= False info= {}\n",
            "step i 20 action= 0\n",
            "obs= [-0.02879539 -0.15028756 -0.02670484  0.13027781] reward= 1.0 done= False info= {}\n",
            "step i 21 action= 1\n",
            "obs= [-0.03180114  0.04520656 -0.02409928 -0.1707091 ] reward= 1.0 done= False info= {}\n",
            "step i 22 action= 0\n",
            "obs= [-0.03089701 -0.14956232 -0.02751347  0.11427488] reward= 1.0 done= False info= {}\n",
            "step i 23 action= 1\n",
            "obs= [-0.03388825  0.04594284 -0.02522797 -0.18695981] reward= 1.0 done= False info= {}\n",
            "step i 24 action= 0\n",
            "obs= [-0.0329694  -0.14880925 -0.02896716  0.09765913] reward= 1.0 done= False info= {}\n",
            "step i 25 action= 1\n",
            "obs= [-0.03594558  0.04671563 -0.02701398 -0.2040203 ] reward= 1.0 done= False info= {}\n",
            "step i 26 action= 0\n",
            "obs= [-0.03501127 -0.14800979 -0.03109439  0.08002012] reward= 1.0 done= False info= {}\n",
            "step i 27 action= 1\n",
            "obs= [-0.03797146  0.04754379 -0.02949399 -0.22230873] reward= 1.0 done= False info= {}\n",
            "step i 28 action= 0\n",
            "obs= [-0.03702059 -0.14714446 -0.03394016  0.06092665] reward= 1.0 done= False info= {}\n",
            "step i 29 action= 1\n",
            "obs= [-0.03996348  0.04844727 -0.03272163 -0.24226847] reward= 1.0 done= False info= {}\n",
            "step i 30 action= 0\n",
            "obs= [-0.03899453 -0.14619237 -0.037567    0.03991626] reward= 1.0 done= False info= {}\n",
            "step i 31 action= 1\n",
            "obs= [-0.04191838  0.04944759 -0.03676867 -0.26437872] reward= 1.0 done= False info= {}\n",
            "step i 32 action= 0\n",
            "obs= [-0.04092943 -0.14513079 -0.04205625  0.01648402] reward= 1.0 done= False info= {}\n",
            "step i 33 action= 1\n",
            "obs= [-0.04383204  0.05056828 -0.04172657 -0.28916591] reward= 1.0 done= False info= {}\n",
            "step i 34 action= 0\n",
            "obs= [-0.04282068 -0.14393459 -0.04750988 -0.00992961] reward= 1.0 done= False info= {}\n",
            "step i 35 action= 0\n",
            "obs= [-0.04569937 -0.33834412 -0.04770848  0.26739299] reward= 1.0 done= False info= {}\n",
            "step i 36 action= 1\n",
            "obs= [-0.05246625 -0.14257491 -0.04236062 -0.03994768] reward= 1.0 done= False info= {}\n",
            "step i 37 action= 0\n",
            "obs= [-0.05531775 -0.33706462 -0.04315957  0.23907488] reward= 1.0 done= False info= {}\n",
            "step i 38 action= 1\n",
            "obs= [-0.06205904 -0.14135353 -0.03837807 -0.06690335] reward= 1.0 done= False info= {}\n",
            "step i 39 action= 0\n",
            "obs= [-0.06488611 -0.33590483 -0.03971614  0.21342833] reward= 1.0 done= False info= {}\n",
            "step i 40 action= 1\n",
            "obs= [-0.07160421 -0.14023823 -0.03544757 -0.0915136 ] reward= 1.0 done= False info= {}\n",
            "step i 41 action= 0\n",
            "obs= [-0.07440897 -0.33483464 -0.03727784  0.18977825] reward= 1.0 done= False info= {}\n",
            "step i 42 action= 1\n",
            "obs= [-0.08110567 -0.13919976 -0.03348228 -0.11442735] reward= 1.0 done= False info= {}\n",
            "step i 43 action= 0\n",
            "obs= [-0.08388966 -0.33382635 -0.03577083  0.16750696] reward= 1.0 done= False info= {}\n",
            "step i 44 action= 1\n",
            "obs= [-0.09056619 -0.13821112 -0.03242069 -0.13624256] reward= 1.0 done= False info= {}\n",
            "step i 45 action= 0\n",
            "obs= [-0.09333041 -0.33285406 -0.03514554  0.1460384 ] reward= 1.0 done= False info= {}\n",
            "step i 46 action= 1\n",
            "obs= [-0.09998749 -0.13724688 -0.03222477 -0.15752182] reward= 1.0 done= False info= {}\n",
            "step i 47 action= 0\n",
            "obs= [-0.10273243 -0.331893   -0.03537521  0.12482333] reward= 1.0 done= False info= {}\n",
            "step i 48 action= 1\n",
            "obs= [-0.10937029 -0.13628258 -0.03287874 -0.17880687] reward= 1.0 done= False info= {}\n",
            "step i 49 action= 0\n",
            "obs= [-0.11209594 -0.33091898 -0.03645488  0.10332533] reward= 1.0 done= False info= {}\n",
            "step i 50 action= 1\n",
            "obs= [-0.11871432 -0.13529407 -0.03438837 -0.20063244] reward= 1.0 done= False info= {}\n",
            "step i 51 action= 0\n",
            "obs= [-0.1214202  -0.32990775 -0.03840102  0.0810073 ] reward= 1.0 done= False info= {}\n",
            "step i 52 action= 1\n",
            "obs= [-0.12801836 -0.13425696 -0.03678087 -0.22353965] reward= 1.0 done= False info= {}\n",
            "step i 53 action= 0\n",
            "obs= [-0.1307035  -0.32883443 -0.04125167  0.05731802] reward= 1.0 done= False info= {}\n",
            "step i 54 action= 1\n",
            "obs= [-0.13728019 -0.13314603 -0.04010531 -0.24808942] reward= 1.0 done= False info= {}\n",
            "step i 55 action= 0\n",
            "obs= [-0.13994311 -0.32767295 -0.04506709  0.03167854] reward= 1.0 done= False info= {}\n",
            "step i 56 action= 1\n",
            "obs= [-0.14649656 -0.13193466 -0.04443352 -0.27487603] reward= 1.0 done= False info= {}\n",
            "step i 57 action= 0\n",
            "obs= [-0.14913526 -0.32639541 -0.04993104  0.00346804] reward= 1.0 done= False info= {}\n",
            "step i 58 action= 1\n",
            "obs= [-0.15566317 -0.13059425 -0.04986168 -0.30454129] reward= 1.0 done= False info= {}\n",
            "step i 59 action= 0\n",
            "obs= [-0.15827505 -0.32497148 -0.05595251 -0.02799108] reward= 1.0 done= False info= {}\n",
            "step i 60 action= 0\n",
            "obs= [-0.16477448 -0.51924825 -0.05651233  0.24652658] reward= 1.0 done= False info= {}\n",
            "step i 61 action= 1\n",
            "obs= [-0.17515945 -0.32336662 -0.0515818  -0.06343258] reward= 1.0 done= False info= {}\n",
            "step i 62 action= 0\n",
            "obs= [-0.18162678 -0.51771253 -0.05285045  0.21254021] reward= 1.0 done= False info= {}\n",
            "step i 63 action= 1\n",
            "obs= [-0.19198103 -0.32187635 -0.04859965 -0.0963347 ] reward= 1.0 done= False info= {}\n",
            "step i 64 action= 0\n",
            "obs= [-0.19841856 -0.51626928 -0.05052634  0.18062774] reward= 1.0 done= False info= {}\n",
            "step i 65 action= 1\n",
            "obs= [-0.20874394 -0.32046209 -0.04691379 -0.12755664] reward= 1.0 done= False info= {}\n",
            "step i 66 action= 0\n",
            "obs= [-0.21515318 -0.51488167 -0.04946492  0.14996428] reward= 1.0 done= False info= {}\n",
            "step i 67 action= 1\n",
            "obs= [-0.22545082 -0.31908759 -0.04646563 -0.15790438] reward= 1.0 done= False info= {}\n",
            "step i 68 action= 0\n",
            "obs= [-0.23183257 -0.51351455 -0.04962372  0.1197653 ] reward= 1.0 done= False info= {}\n",
            "step i 69 action= 1\n",
            "obs= [-0.24210286 -0.31771804 -0.04722841 -0.18815132] reward= 1.0 done= False info= {}\n",
            "step i 70 action= 0\n",
            "obs= [-0.24845722 -0.51213359 -0.05099144  0.08926685] reward= 1.0 done= False info= {}\n",
            "step i 71 action= 1\n",
            "obs= [-0.25869989 -0.31631924 -0.0492061  -0.2190579 ] reward= 1.0 done= False info= {}\n",
            "step i 72 action= 0\n",
            "obs= [-0.26502628 -0.51070453 -0.05358726  0.05770636] reward= 1.0 done= False info= {}\n",
            "step i 73 action= 1\n",
            "obs= [-0.27524037 -0.31485682 -0.05243313 -0.25139063] reward= 1.0 done= False info= {}\n",
            "step i 74 action= 0\n",
            "obs= [-0.2815375  -0.50919235 -0.05746095  0.02430378] reward= 1.0 done= False info= {}\n",
            "step i 75 action= 1\n",
            "obs= [-0.29172135 -0.31329545 -0.05697487 -0.28594083] reward= 1.0 done= False info= {}\n",
            "step i 76 action= 0\n",
            "obs= [-0.29798726 -0.50756051 -0.06269369 -0.01175761] reward= 1.0 done= False info= {}\n",
            "step i 77 action= 0\n",
            "obs= [-0.30813847 -0.70172994 -0.06292884  0.26050445] reward= 1.0 done= False info= {}\n",
            "step i 78 action= 1\n",
            "obs= [-0.32217307 -0.50576874 -0.05771875 -0.05134439] reward= 1.0 done= False info= {}\n",
            "step i 79 action= 0\n",
            "obs= [-0.33228844 -0.70001761 -0.05874564  0.22258381] reward= 1.0 done= False info= {}\n",
            "step i 80 action= 1\n",
            "obs= [-0.3462888  -0.50410735 -0.05429396 -0.08803595] reward= 1.0 done= False info= {}\n",
            "step i 81 action= 0\n",
            "obs= [-0.35637094 -0.69841072 -0.05605468  0.18703504] reward= 1.0 done= False info= {}\n",
            "step i 82 action= 1\n",
            "obs= [-0.37033916 -0.50253344 -0.05231398 -0.12279083] reward= 1.0 done= False info= {}\n",
            "step i 83 action= 0\n",
            "obs= [-0.38038983 -0.69686839 -0.0547698   0.1529395 ] reward= 1.0 done= False info= {}\n",
            "step i 84 action= 1\n",
            "obs= [-0.39432719 -0.50100673 -0.05171101 -0.15650673] reward= 1.0 done= False info= {}\n",
            "step i 85 action= 0\n",
            "obs= [-0.40434733 -0.69535163 -0.05484114  0.11942469] reward= 1.0 done= False info= {}\n",
            "step i 86 action= 1\n",
            "obs= [-0.41825436 -0.49948858 -0.05245265 -0.19004342] reward= 1.0 done= False info= {}\n",
            "step i 87 action= 0\n",
            "obs= [-0.42824413 -0.69382242 -0.05625352  0.08564243] reward= 1.0 done= False info= {}\n",
            "step i 88 action= 1\n",
            "obs= [-0.44212058 -0.49794112 -0.05454067 -0.22424456] reward= 1.0 done= False info= {}\n",
            "step i 89 action= 0\n",
            "obs= [-0.4520794  -0.69224285 -0.05902556  0.05074765] reward= 1.0 done= False info= {}\n",
            "step i 90 action= 1\n",
            "obs= [-0.46592426 -0.49632639 -0.05801061 -0.2599587 ] reward= 1.0 done= False info= {}\n",
            "step i 91 action= 0\n",
            "obs= [-0.47585079 -0.69057429 -0.06320978  0.01387747] reward= 1.0 done= False info= {}\n",
            "step i 92 action= 1\n",
            "obs= [-0.48966227 -0.49460546 -0.06293223 -0.29806003] reward= 1.0 done= False info= {}\n",
            "step i 93 action= 0\n",
            "obs= [-0.49955438 -0.68877651 -0.06889343 -0.02586989] reward= 1.0 done= False info= {}\n",
            "step i 94 action= 0\n",
            "obs= [-0.51332991 -0.88284631 -0.06941083  0.24430559] reward= 1.0 done= False info= {}\n",
            "step i 95 action= 1\n",
            "obs= [-0.53098684 -0.68680513 -0.06452472 -0.06943848] reward= 1.0 done= False info= {}\n",
            "step i 96 action= 0\n",
            "obs= [-0.54472294 -0.88094551 -0.06591349  0.20220898] reward= 1.0 done= False info= {}\n",
            "step i 97 action= 1\n",
            "obs= [-0.56234185 -0.68494585 -0.06186931 -0.11051664] reward= 1.0 done= False info= {}\n",
            "step i 98 action= 0\n",
            "obs= [-0.57604077 -0.87912918 -0.06407964  0.1620231 ] reward= 1.0 done= False info= {}\n",
            "step i 99 action= 1\n",
            "obs= [-0.59362335 -0.68315119 -0.06083918 -0.15016708] reward= 1.0 done= False info= {}\n",
            "step i 100 action= 0\n",
            "obs= [-0.60728638 -0.87735158 -0.06384252  0.12271887] reward= 1.0 done= False info= {}\n",
            "step i 101 action= 1\n",
            "obs= [-0.62483341 -0.68137582 -0.06138814 -0.18940284] reward= 1.0 done= False info= {}\n",
            "step i 102 action= 0\n",
            "obs= [-0.63846092 -0.87556825 -0.0651762   0.08330034] reward= 1.0 done= False info= {}\n",
            "step i 103 action= 1\n",
            "obs= [-0.65597229 -0.6795755  -0.06351019 -0.22921283] reward= 1.0 done= False info= {}\n",
            "step i 104 action= 0\n",
            "obs= [-0.6695638  -0.8737351  -0.06809445  0.04277995] reward= 1.0 done= False info= {}\n",
            "step i 105 action= 1\n",
            "obs= [-0.6870385  -0.67770611 -0.06723885 -0.27058637] reward= 1.0 done= False info= {}\n",
            "step i 106 action= 0\n",
            "obs= [-7.00592624e-01 -8.71807352e-01 -7.26505778e-02  1.54249399e-04] reward= 1.0 done= False info= {}\n",
            "step i 107 action= 1\n",
            "obs= [-0.71802877 -0.67572274 -0.07264749 -0.31453727] reward= 1.0 done= False info= {}\n",
            "step i 108 action= 0\n",
            "obs= [-0.73154323 -0.8697387  -0.07893824 -0.04562055] reward= 1.0 done= False info= {}\n",
            "step i 109 action= 0\n",
            "obs= [-0.748938   -1.06364519 -0.07985065  0.22114971] reward= 1.0 done= False info= {}\n",
            "step i 110 action= 1\n",
            "obs= [-0.7702109  -0.86747806 -0.07542766 -0.09561455] reward= 1.0 done= False info= {}\n",
            "step i 111 action= 0\n",
            "obs= [-0.78756046 -1.06144244 -0.07733995  0.17235006] reward= 1.0 done= False info= {}\n",
            "step i 112 action= 1\n",
            "obs= [-0.80878931 -0.86530365 -0.07389294 -0.14369394] reward= 1.0 done= False info= {}\n",
            "step i 113 action= 0\n",
            "obs= [-0.82609539 -1.05929385 -0.07676682  0.12479254] reward= 1.0 done= False info= {}\n",
            "step i 114 action= 1\n",
            "obs= [-0.84728126 -0.86316084 -0.07427097 -0.19108781] reward= 1.0 done= False info= {}\n",
            "step i 115 action= 0\n",
            "obs= [-0.86454448 -1.05714604 -0.07809273  0.07727222] reward= 1.0 done= False info= {}\n",
            "step i 116 action= 1\n",
            "obs= [-0.8856874  -0.86099648 -0.07654728 -0.23899135] reward= 1.0 done= False info= {}\n",
            "step i 117 action= 0\n",
            "obs= [-0.90290733 -1.05494618 -0.08132711  0.02859836] reward= 1.0 done= False info= {}\n",
            "step i 118 action= 1\n",
            "obs= [-0.92400625 -0.85875782 -0.08075514 -0.28859534] reward= 1.0 done= False info= {}\n",
            "step i 119 action= 0\n",
            "obs= [-0.94118141 -1.05264089 -0.08652705 -0.02243472] reward= 1.0 done= False info= {}\n",
            "step i 120 action= 0\n",
            "obs= [-0.96223423 -1.24642225 -0.08697575  0.24174266] reward= 1.0 done= False info= {}\n",
            "step i 121 action= 1\n",
            "obs= [-0.98716267 -1.05017253 -0.08214089 -0.07705783] reward= 1.0 done= False info= {}\n",
            "step i 122 action= 0\n",
            "obs= [-1.00816613 -1.24402674 -0.08368205  0.18862079] reward= 1.0 done= False info= {}\n",
            "step i 123 action= 1\n",
            "obs= [-1.03304666 -1.04781347 -0.07990963 -0.12924303] reward= 1.0 done= False info= {}\n",
            "step i 124 action= 0\n",
            "obs= [-1.05400293 -1.24170517 -0.08249449  0.137198  ] reward= 1.0 done= False info= {}\n",
            "step i 125 action= 1\n",
            "obs= [-1.07883703 -1.04550448 -0.07975053 -0.18032807] reward= 1.0 done= False info= {}\n",
            "step i 126 action= 0\n",
            "obs= [-1.09974712 -1.23940002 -0.0833571   0.08616901] reward= 1.0 done= False info= {}\n",
            "step i 127 action= 1\n",
            "obs= [-1.12453512 -1.04318826 -0.08163372 -0.23160531] reward= 1.0 done= False info= {}\n",
            "step i 128 action= 0\n",
            "obs= [-1.14539889 -1.23705461 -0.08626582  0.03425213] reward= 1.0 done= False info= {}\n",
            "step i 129 action= 1\n",
            "obs= [-1.17013998 -1.0408083  -0.08558078 -0.2843534 ] reward= 1.0 done= False info= {}\n",
            "step i 130 action= 0\n",
            "obs= [-1.19095615 -1.23461202 -0.09126785 -0.0198418 ] reward= 1.0 done= False info= {}\n",
            "step i 131 action= 0\n",
            "obs= [-1.21564839 -1.42831461 -0.09166468  0.24270729] reward= 1.0 done= False info= {}\n",
            "step i 132 action= 1\n",
            "obs= [-1.24421468 -1.23201102 -0.08681054 -0.07742359] reward= 1.0 done= False info= {}\n",
            "step i 133 action= 0\n",
            "obs= [-1.2688549  -1.42578815 -0.08835901  0.1866573 ] reward= 1.0 done= False info= {}\n",
            "step i 134 action= 1\n",
            "obs= [-1.29737066 -1.22952041 -0.08462586 -0.13253956] reward= 1.0 done= False info= {}\n",
            "step i 135 action= 0\n",
            "obs= [-1.32196107 -1.42333462 -0.08727665  0.13229105] reward= 1.0 done= False info= {}\n",
            "step i 136 action= 1\n",
            "obs= [-1.35042776 -1.22707794 -0.08463083 -0.18660026] reward= 1.0 done= False info= {}\n",
            "step i 137 action= 0\n",
            "obs= [-1.37496932 -1.42089348 -0.08836284  0.07823077] reward= 1.0 done= False info= {}\n",
            "step i 138 action= 1\n",
            "obs= [-1.40338719 -1.22462324 -0.08679822 -0.24097088] reward= 1.0 done= False info= {}\n",
            "step i 139 action= 0\n",
            "obs= [-1.42787966 -1.41840497 -0.09161764  0.02312081] reward= 1.0 done= False info= {}\n",
            "step i 140 action= 1\n",
            "obs= [-1.45624775 -1.22209671 -0.09115522 -0.29700454] reward= 1.0 done= False info= {}\n",
            "step i 141 action= 0\n",
            "obs= [-1.48068969 -1.41580897 -0.09709531 -0.03440505] reward= 1.0 done= False info= {}\n",
            "step i 142 action= 0\n",
            "obs= [-1.50900587 -1.60941415 -0.09778342  0.22613369] reward= 1.0 done= False info= {}\n",
            "step i 143 action= 1\n",
            "obs= [-1.54119415 -1.41304058 -0.09326074 -0.09572208] reward= 1.0 done= False info= {}\n",
            "step i 144 action= 0\n",
            "obs= [-1.56945496 -1.60671072 -0.09517518  0.16614177] reward= 1.0 done= False info= {}\n",
            "step i 145 action= 1\n",
            "obs= [-1.60158918 -1.41036435 -0.09185235 -0.15498415] reward= 1.0 done= False info= {}\n",
            "step i 146 action= 0\n",
            "obs= [-1.62979646 -1.60405931 -0.09495203  0.10736689] reward= 1.0 done= False info= {}\n",
            "step i 147 action= 1\n",
            "obs= [-1.66187765 -1.40771399 -0.09280469 -0.21369839] reward= 1.0 done= False info= {}\n",
            "step i 148 action= 0\n",
            "obs= [-1.69003193 -1.60139503 -0.09707866  0.04832756] reward= 1.0 done= False info= {}\n",
            "step i 149 action= 1\n",
            "obs= [-1.72205983 -1.40502477 -0.09611211 -0.27333726] reward= 1.0 done= False info= {}\n",
            "step i 150 action= 0\n",
            "obs= [-1.75016033 -1.59865333 -0.10157885 -0.01244836] reward= 1.0 done= False info= {}\n",
            "step i 151 action= 0\n",
            "obs= [-1.78213339 -1.79218289 -0.10182782  0.24653676] reward= 1.0 done= False info= {}\n",
            "step i 152 action= 1\n",
            "obs= [-1.81797705 -1.59576519 -0.09689709 -0.0764493 ] reward= 1.0 done= False info= {}\n",
            "step i 153 action= 0\n",
            "obs= [-1.84989235 -1.78937414 -0.09842607  0.18415864] reward= 1.0 done= False info= {}\n",
            "step i 154 action= 1\n",
            "obs= [-1.88567984 -1.59299172 -0.0947429  -0.13787983] reward= 1.0 done= False info= {}\n",
            "step i 155 action= 0\n",
            "obs= [-1.91753967 -1.78663798 -0.0975005   0.12347412] reward= 1.0 done= False info= {}\n",
            "step i 156 action= 1\n",
            "obs= [-1.95327243 -1.59026419 -0.09503101 -0.19830733] reward= 1.0 done= False info= {}\n",
            "step i 157 action= 0\n",
            "obs= [-1.98507772 -1.78390751 -0.09899716  0.06294998] reward= 1.0 done= False info= {}\n",
            "step i 158 action= 1\n",
            "obs= [-2.02075587 -1.58751581 -0.09773816 -0.25925286] reward= 1.0 done= False info= {}\n",
            "step i 159 action= 0\n",
            "obs= [-2.05250618e+00 -1.78111647e+00 -1.02923219e-01  1.07287631e-03] reward= 1.0 done= False info= {}\n",
            "step i 160 action= 1\n",
            "obs= [-2.08812851 -1.58468057 -0.10290176 -0.32222771] reward= 1.0 done= False info= {}\n",
            "step i 161 action= 0\n",
            "obs= [-2.11982212 -1.77819816 -0.10934632 -0.06368657] reward= 1.0 done= False info= {}\n",
            "step i 162 action= 0\n",
            "obs= [-2.15538609 -1.9715963  -0.11062005  0.19259428] reward= 1.0 done= False info= {}\n",
            "step i 163 action= 1\n",
            "obs= [-2.19481801 -1.77508004 -0.10676816 -0.1328344 ] reward= 1.0 done= False info= {}\n",
            "step i 164 action= 0\n",
            "obs= [-2.23031961 -1.96852348 -0.10942485  0.12434824] reward= 1.0 done= False info= {}\n",
            "step i 165 action= 1\n",
            "obs= [-2.26969008 -1.77201784 -0.10693788 -0.20075404] reward= 1.0 done= False info= {}\n",
            "step i 166 action= 0\n",
            "obs= [-2.30513044 -1.96546076 -0.11095297  0.05637295] reward= 1.0 done= False info= {}\n",
            "step i 167 action= 1\n",
            "obs= [-2.34443965 -1.76893725 -0.10982551 -0.26915297] reward= 1.0 done= False info= {}\n",
            "step i 168 action= 0\n",
            "obs= [-2.3798184  -1.96233452 -0.11520857 -0.01302865] reward= 1.0 done= False info= {}\n",
            "step i 169 action= 0\n",
            "obs= [-2.41906509 -2.15563192 -0.11546914  0.2411989 ] reward= 1.0 done= True info= {}\n"
          ]
        }
      ]
    }
  ]
}