{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOHLFkcdoUDm"
   },
   "source": [
    "# A2: Monte Carlo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtzvN4e9onbh"
   },
   "source": [
    "## Monte Carlo Part A\n",
    ">1. Explain clearly why V_pi is not useful in the MC development above?\n",
    "\n",
    "Because state values V_pi alone simply leads us to the best combination of reward and next state S' following the policy pi. However, without the the transition dynamics p, we don't know what action can take us to state S'. \n",
    "\n",
    ">2. The MC algorithm so far (ref: p 99), requires an infinite number of episodes for Eval to converge on Q_pi_k (step k). We can modify this algorithm to the practical variant where Eval is truncated (c.f., DynProg GPI). In this case: \n",
    ">* a. Will we obtain Q_pi_k from eval?\n",
    "\n",
    "No. Since evaluation is truncated, we can only obtain an approximate value.\n",
    ">* b. If not why are we able to truncate Eval? Explain clearly.\n",
    "\n",
    "Because by implementing truncated evaluation and improvement repeatedly, the value function is altered to more closely approximate to true value function for the current policy, and the policy is improved concerning the current value function. These two kinds of changes together cause both the value function and the policy to converge to the optimality.\n",
    "\n",
    "\n",
    ">* c. Assuming ES (i.e., thorough sampling of the S x A space), and the above truncated Eval_trunc, is it possible to converge on a sub-optimal policy pi_c? Is this a stable fixed point of the GPI for MC? Explain clearly.\n",
    "\n",
    "It cannot converge to any sub-optimal policy. Because if it converges on a sub-optimal policy pi_c, then throughout policy evaluation repeatedly, the value function will converge to the real value function under policy pi_c. Then policy improvement will guarantee a new policy better than policy pi_c. So it will eventually lead to the optimal policy.\n",
    ">3. Explain how you can synthesize a stochastic policy given what you know so far (you don't need to read ahead).\n",
    "\n",
    "Start with a policy with non-zero values for each state-action pair so that all state-action pairs will be visited in a finite number of episodes.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEWmnr0Yq4g5"
   },
   "source": [
    "##Monte Carlo Part B-1: Stochastic Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ1jaXzqrEbt"
   },
   "source": [
    "##Monte Carlo Part B-2: Off Policy Methods\n",
    ">Code the algorithm for MC Control (Off Policy) and apply this to the Cart Pole problem. You must discretize the environmental feedback (S) in order to solve this problem properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "pFIKbYsZoPT9"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "meKtIgZVD1cr",
    "outputId": "ddf4b6c0-04ca-48fe-9087-0d0c315287a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 0 action= 0\n",
      "obs= [ 0.02728512 -0.15698186  0.03223856  0.28679774] reward= 1.0 done= False info= {}\n",
      "step i 1 action= 0\n",
      "obs= [ 0.02414549 -0.35254838  0.03797451  0.58947159] reward= 1.0 done= False info= {}\n",
      "step i 2 action= 1\n",
      "obs= [ 0.01709452 -0.15797817  0.04976394  0.30898851] reward= 1.0 done= False info= {}\n",
      "step i 3 action= 1\n",
      "obs= [0.01393495 0.03640071 0.05594371 0.0324057 ] reward= 1.0 done= False info= {}\n",
      "step i 4 action= 0\n",
      "obs= [ 0.01466297 -0.15947701  0.05659183  0.3422015 ] reward= 1.0 done= False info= {}\n",
      "step i 5 action= 1\n",
      "obs= [0.01147343 0.03479605 0.06343586 0.06788755] reward= 1.0 done= False info= {}\n",
      "step i 6 action= 1\n",
      "obs= [ 0.01216935  0.22895387  0.06479361 -0.20412576] reward= 1.0 done= False info= {}\n",
      "step i 7 action= 1\n",
      "obs= [ 0.01674843  0.42309226  0.06071109 -0.47568629] reward= 1.0 done= False info= {}\n",
      "step i 8 action= 1\n",
      "obs= [ 0.02521027  0.61730674  0.05119737 -0.7486332 ] reward= 1.0 done= False info= {}\n",
      "step i 9 action= 0\n",
      "obs= [ 0.03755641  0.42151733  0.0362247  -0.44028844] reward= 1.0 done= False info= {}\n",
      "step i 10 action= 0\n",
      "obs= [ 0.04598675  0.22590194  0.02741893 -0.13641013] reward= 1.0 done= False info= {}\n",
      "step i 11 action= 0\n",
      "obs= [0.05050479 0.03039821 0.02469073 0.1647954 ] reward= 1.0 done= False info= {}\n",
      "step i 12 action= 0\n",
      "obs= [ 0.05111276 -0.16506833  0.02798664  0.46516418] reward= 1.0 done= False info= {}\n",
      "step i 13 action= 1\n",
      "obs= [0.04781139 0.02964721 0.03728992 0.18143224] reward= 1.0 done= False info= {}\n",
      "step i 14 action= 1\n",
      "obs= [ 0.04840433  0.22421628  0.04091857 -0.09925777] reward= 1.0 done= False info= {}\n",
      "step i 15 action= 0\n",
      "obs= [0.05288866 0.0285325  0.03893341 0.2060489 ] reward= 1.0 done= False info= {}\n",
      "step i 16 action= 0\n",
      "obs= [ 0.05345931 -0.16712394  0.04305439  0.5107547 ] reward= 1.0 done= False info= {}\n",
      "step i 17 action= 0\n",
      "obs= [ 0.05011683 -0.36282511  0.05326948  0.8166885 ] reward= 1.0 done= False info= {}\n",
      "step i 18 action= 0\n",
      "obs= [ 0.04286033 -0.55863432  0.06960325  1.1256395 ] reward= 1.0 done= False info= {}\n",
      "step i 19 action= 0\n",
      "obs= [ 0.03168764 -0.75459594  0.09211604  1.43931703] reward= 1.0 done= False info= {}\n",
      "step i 20 action= 1\n",
      "obs= [ 0.01659572 -0.56072172  0.12090238  1.1767825 ] reward= 1.0 done= False info= {}\n",
      "step i 21 action= 0\n",
      "obs= [ 0.00538129 -0.75718869  0.14443803  1.50479045] reward= 1.0 done= False info= {}\n",
      "step i 22 action= 0\n",
      "obs= [-0.00976249 -0.95373747  0.17453384  1.8388609 ] reward= 1.0 done= False info= {}\n",
      "step i 23 action= 1\n",
      "obs= [-0.02883723 -0.76092064  0.21131106  1.60508251] reward= 1.0 done= True info= {}\n",
      "Iterations that were run: 23\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "for i in range(50000):\n",
    "  action = env.action_space.sample()\n",
    "  print(\"step i\",i,\"action=\",action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
    "\n",
    "  if done:\n",
    "    break\n",
    "    \n",
    "env.close()\n",
    "print(\"Iterations that were run:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "DwgUTJ_4Ekey"
   },
   "outputs": [],
   "source": [
    "#discretize the environmental feedback (S)\n",
    "'''\n",
    "  Type: Box(4)\n",
    "  Num     Observation               Min                     Max\n",
    "  0       Cart Position             -4.8                    4.8\n",
    "  1       Cart Velocity             -Inf                    Inf\n",
    "  2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "  3       Pole Angular Velocity     -Inf                    Inf\n",
    "'''\n",
    "\n",
    "def discretize_state(obs):\n",
    "\t# env.observation_space.high\n",
    "\t# [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
    "\t# env.observation_space.low\n",
    "\t# [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
    "  discrete = [np.digitize(obs[i], bins) for i, bins in enumerate([\n",
    "    np.linspace(-4.8, 4.8, 9),\n",
    "    np.linspace(-4, 4, 7),\n",
    "    np.linspace(-0.418, 0.418, 9),\n",
    "    np.linspace(-4, 4, 7),\n",
    "    ])]\n",
    "  return ((obs > 0) * 8 **np.arange(len(obs))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "mtCw0IOHENAU"
   },
   "outputs": [],
   "source": [
    "#policy initialization\n",
    "def init_policy(pi,eps,S,A):\n",
    "  b = np.full((S, A), fill_value=1.0*eps/A)\n",
    "  b[np.arange(S), pi] += 1 - eps\n",
    "  return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "RBdqdtqxPCLb"
   },
   "outputs": [],
   "source": [
    "#generate episode using b\n",
    "def generate_episode(env, policy, actions):\n",
    "  episode = []\n",
    "  obs = env.reset()\n",
    "  i = 0\n",
    "  while True:\n",
    "    i += 1\n",
    "    state = discretize_state(obs)\n",
    "    action = np.random.choice(actions, p=policy[state])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode.append((state, action, reward))\n",
    "    if done:\n",
    "      break\n",
    "  return episode,i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "7XjIwb9REEhX"
   },
   "outputs": [],
   "source": [
    "#MC Control (Off Policy)\n",
    "def Monte_Carlo_off_policy(env,gamma):\n",
    "  S = 8**len(obs)\n",
    "  A = env.action_space.n\n",
    "  #initialize\n",
    "  states = np.arange(S)\n",
    "  actions = np.arange(A)\n",
    "  Q = np.random.random([S,A])\n",
    "  C = np.zeros([S,A])\n",
    "  T_ep = []\n",
    "  #initialize pi\n",
    "  pi = np.argmax(Q, axis = 1)\n",
    "\n",
    "\n",
    "  for episode in range(2000):\n",
    "    if episode % 100 == 1:\n",
    "\n",
    "        b_iters = np.mean(T_ep[-100:])\n",
    "        \n",
    "        print(f\"Learning iter {episode}, \" +\n",
    "        f\"off-policy average {b_iters}, \" +\n",
    "        f\"greedy policy lasted {i} iterations, \" + \n",
    "        f\"eps = {epsilon}\")\n",
    "\n",
    "    epsilon = 0.1 - 0.099/2000 * episode\n",
    "    b = init_policy(pi, epsilon, S, A)\n",
    "    #generate episodes\n",
    "    episodes, i = generate_episode(env,b,actions)\n",
    "    \n",
    "    G = 0\n",
    "    W = 1\n",
    "    T_ep.append(i)\n",
    "    for t in range(i-1,-1,-1):\n",
    "      state, action, reward = episodes[t]\n",
    "      G = gamma * G + reward\n",
    "      C[state,action] += W\n",
    "      Q[state,action] += W/C[state,action]*(G-Q[state,action])\n",
    "      pi[state] = np.argmax(Q[state]) \n",
    "      if action != pi[state]:\n",
    "        break\n",
    "      W = W / b[state,action]\n",
    "\n",
    "  return Q, pi, T_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3J0soooNdce",
    "outputId": "317029e9-403d-4b29-eb42-e2941a0e9f08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning iter 1, off-policy average 28.0, greedy policy lasted 28 iterations, eps = 0.1\n",
      "Learning iter 101, off-policy average 61.32, greedy policy lasted 9 iterations, eps = 0.09505000000000001\n",
      "Learning iter 201, off-policy average 90.7, greedy policy lasted 169 iterations, eps = 0.0901\n",
      "Learning iter 301, off-policy average 112.71, greedy policy lasted 137 iterations, eps = 0.08515\n",
      "Learning iter 401, off-policy average 113.01, greedy policy lasted 10 iterations, eps = 0.08020000000000001\n",
      "Learning iter 501, off-policy average 116.69, greedy policy lasted 172 iterations, eps = 0.07525000000000001\n",
      "Learning iter 601, off-policy average 113.47, greedy policy lasted 200 iterations, eps = 0.0703\n",
      "Learning iter 701, off-policy average 118.38, greedy policy lasted 155 iterations, eps = 0.06535\n",
      "Learning iter 801, off-policy average 148.43, greedy policy lasted 96 iterations, eps = 0.0604\n",
      "Learning iter 901, off-policy average 142.46, greedy policy lasted 178 iterations, eps = 0.05545\n",
      "Learning iter 1001, off-policy average 140.71, greedy policy lasted 185 iterations, eps = 0.0505\n",
      "Learning iter 1101, off-policy average 147.32, greedy policy lasted 117 iterations, eps = 0.04555\n",
      "Learning iter 1201, off-policy average 140.19, greedy policy lasted 184 iterations, eps = 0.040600000000000004\n",
      "Learning iter 1301, off-policy average 150.49, greedy policy lasted 200 iterations, eps = 0.03565\n",
      "Learning iter 1401, off-policy average 151.44, greedy policy lasted 200 iterations, eps = 0.030700000000000005\n",
      "Learning iter 1501, off-policy average 164.74, greedy policy lasted 200 iterations, eps = 0.025749999999999995\n",
      "Learning iter 1601, off-policy average 155.34, greedy policy lasted 140 iterations, eps = 0.0208\n",
      "Learning iter 1701, off-policy average 158.36, greedy policy lasted 163 iterations, eps = 0.015850000000000003\n",
      "Learning iter 1801, off-policy average 166.25, greedy policy lasted 200 iterations, eps = 0.010899999999999993\n",
      "Learning iter 1901, off-policy average 156.46, greedy policy lasted 185 iterations, eps = 0.005949999999999997\n"
     ]
    }
   ],
   "source": [
    "Q,pi,T_ep = Monte_Carlo_off_policy(env,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "oj1eNgpX6zHH",
    "outputId": "2281be3c-a4a3-41d6-a9b7-766c36eaf917"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(T_ep, '.', markersize=1)\n",
    "plt.title(\"Performance of off-policy\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Length of training episodes\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e6rDSaza_uv"
   },
   "source": [
    "As shown in the above figure, we can keep the pole upright for an average of 150 to 160 steps and the steps are mostly above 100. We can visualize the performance of the policy has improved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uuq8Nf2APqe0",
    "outputId": "c91d232e-cccb-4128-9b3b-468bdd22a585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 0 action= 0\n",
      "obs= [ 0.03198521 -0.14631241 -0.02050291  0.28816424] reward= 1.0 done= False info= {}\n",
      "step i 1 action= 1\n",
      "obs= [ 0.02905896  0.04909583 -0.01473963 -0.01091395] reward= 1.0 done= False info= {}\n",
      "step i 2 action= 0\n",
      "obs= [ 0.03004088 -0.14581166 -0.01495791  0.27708225] reward= 1.0 done= False info= {}\n",
      "step i 3 action= 1\n",
      "obs= [ 0.02712465  0.04952046 -0.00941626 -0.02028062] reward= 1.0 done= False info= {}\n",
      "step i 4 action= 0\n",
      "obs= [ 0.02811506 -0.1454652  -0.00982188  0.26941656] reward= 1.0 done= False info= {}\n",
      "step i 5 action= 1\n",
      "obs= [ 0.02520575  0.04979553 -0.00443354 -0.02634799] reward= 1.0 done= False info= {}\n",
      "step i 6 action= 0\n",
      "obs= [ 0.02620166 -0.14526256 -0.0049605   0.26493281] reward= 1.0 done= False info= {}\n",
      "step i 7 action= 1\n",
      "obs= [ 0.02329641  0.04992984  0.00033815 -0.02931057] reward= 1.0 done= False info= {}\n",
      "step i 8 action= 1\n",
      "obs= [ 2.42950085e-02  2.45046938e-01 -2.48059439e-04 -3.21886784e-01] reward= 1.0 done= False info= {}\n",
      "step i 9 action= 0\n",
      "obs= [ 0.02919595  0.04992852 -0.0066858  -0.0292821 ] reward= 1.0 done= False info= {}\n",
      "step i 10 action= 0\n",
      "obs= [ 0.03019452 -0.14509692 -0.00727144  0.26128391] reward= 1.0 done= False info= {}\n",
      "step i 11 action= 1\n",
      "obs= [ 0.02729258  0.05012807 -0.00204576 -0.03368362] reward= 1.0 done= False info= {}\n",
      "step i 12 action= 0\n",
      "obs= [ 0.02829514 -0.14496448 -0.00271943  0.25835315] reward= 1.0 done= False info= {}\n",
      "step i 13 action= 1\n",
      "obs= [ 0.02539585  0.05019619  0.00244763 -0.03518628] reward= 1.0 done= False info= {}\n",
      "step i 14 action= 1\n",
      "obs= [ 0.02639977  0.24528295  0.00174391 -0.32709595] reward= 1.0 done= False info= {}\n",
      "step i 15 action= 1\n",
      "obs= [ 0.03130543  0.44038003 -0.00479801 -0.61922842] reward= 1.0 done= False info= {}\n",
      "step i 16 action= 0\n",
      "obs= [ 0.04011303  0.24532543 -0.01718258 -0.32806048] reward= 1.0 done= False info= {}\n",
      "step i 17 action= 0\n",
      "obs= [ 0.04501954  0.05045225 -0.02374379 -0.04084531] reward= 1.0 done= False info= {}\n",
      "step i 18 action= 0\n",
      "obs= [ 0.04602859 -0.14432131 -0.0245607   0.24425267] reward= 1.0 done= False info= {}\n",
      "step i 19 action= 1\n",
      "obs= [ 0.04314216  0.05114268 -0.01967564 -0.05607501] reward= 1.0 done= False info= {}\n",
      "step i 20 action= 0\n",
      "obs= [ 0.04416502 -0.14369171 -0.02079714  0.23033574] reward= 1.0 done= False info= {}\n",
      "step i 21 action= 1\n",
      "obs= [ 0.04129118  0.05172116 -0.01619043 -0.06883409] reward= 1.0 done= False info= {}\n",
      "step i 22 action= 0\n",
      "obs= [ 0.0423256  -0.14316498 -0.01756711  0.21869702] reward= 1.0 done= False info= {}\n",
      "step i 23 action= 1\n",
      "obs= [ 0.03946231  0.05220363 -0.01319317 -0.07947514] reward= 1.0 done= False info= {}\n",
      "step i 24 action= 0\n",
      "obs= [ 0.04050638 -0.14272673 -0.01478267  0.20901628] reward= 1.0 done= False info= {}\n",
      "step i 25 action= 1\n",
      "obs= [ 0.03765184  0.05260344 -0.01060235 -0.08829291] reward= 1.0 done= False info= {}\n",
      "step i 26 action= 0\n",
      "obs= [ 0.03870391 -0.14236495 -0.01236821  0.2010262 ] reward= 1.0 done= False info= {}\n",
      "step i 27 action= 1\n",
      "obs= [ 0.03585661  0.05293169 -0.00834768 -0.09553251] reward= 1.0 done= False info= {}\n",
      "step i 28 action= 0\n",
      "obs= [ 0.03691525 -0.14206963 -0.01025833  0.19450508] reward= 1.0 done= False info= {}\n",
      "step i 29 action= 1\n",
      "obs= [ 0.03407385  0.05319755 -0.00636823 -0.10139617] reward= 1.0 done= False info= {}\n",
      "step i 30 action= 0\n",
      "obs= [ 0.03513781 -0.14183256 -0.00839615  0.18927082] reward= 1.0 done= False info= {}\n",
      "step i 31 action= 1\n",
      "obs= [ 0.03230115  0.0534085  -0.00461074 -0.10604889] reward= 1.0 done= False info= {}\n",
      "step i 32 action= 0\n",
      "obs= [ 0.03336932 -0.14164707 -0.00673172  0.18517581] reward= 1.0 done= False info= {}\n",
      "step i 33 action= 1\n",
      "obs= [ 0.03053638  0.05357055 -0.0030282  -0.10962309] reward= 1.0 done= False info= {}\n",
      "step i 34 action= 0\n",
      "obs= [ 0.03160779 -0.14150788 -0.00522066  0.18210292] reward= 1.0 done= False info= {}\n",
      "step i 35 action= 1\n",
      "obs= [ 0.02877764  0.05368838 -0.0015786  -0.11222235] reward= 1.0 done= False info= {}\n",
      "step i 36 action= 0\n",
      "obs= [ 0.0298514  -0.14141091 -0.00382305  0.17996212] reward= 1.0 done= False info= {}\n",
      "step i 37 action= 1\n",
      "obs= [ 0.02702319  0.05376554 -0.00022381 -0.11392439] reward= 1.0 done= False info= {}\n",
      "step i 38 action= 0\n",
      "obs= [ 0.0280985  -0.14135321 -0.0025023   0.17868792] reward= 1.0 done= False info= {}\n",
      "step i 39 action= 1\n",
      "obs= [ 0.02527143  0.05380446  0.00107146 -0.11478335] reward= 1.0 done= False info= {}\n",
      "step i 40 action= 1\n",
      "obs= [ 0.02634752  0.24891104 -0.0012242  -0.40712804] reward= 1.0 done= False info= {}\n",
      "step i 41 action= 0\n",
      "obs= [ 0.03132574  0.05380647 -0.00936676 -0.11483132] reward= 1.0 done= False info= {}\n",
      "step i 42 action= 0\n",
      "obs= [ 0.03240187 -0.14118002 -0.01166339  0.1748818 ] reward= 1.0 done= False info= {}\n",
      "step i 43 action= 1\n",
      "obs= [ 0.02957827  0.05410689 -0.00816575 -0.1214576 ] reward= 1.0 done= False info= {}\n",
      "step i 44 action= 0\n",
      "obs= [ 0.03066041 -0.14089712 -0.01059491  0.16863796] reward= 1.0 done= False info= {}\n",
      "step i 45 action= 1\n",
      "obs= [ 0.02784247  0.05437487 -0.00722215 -0.12736843] reward= 1.0 done= False info= {}\n",
      "step i 46 action= 0\n",
      "obs= [ 0.02892996 -0.14064288 -0.00976952  0.16302727] reward= 1.0 done= False info= {}\n",
      "step i 47 action= 1\n",
      "obs= [ 0.02611711  0.05461755 -0.00650897 -0.13272159] reward= 1.0 done= False info= {}\n",
      "step i 48 action= 0\n",
      "obs= [ 0.02720946 -0.14041056 -0.0091634   0.15790075] reward= 1.0 done= False info= {}\n",
      "step i 49 action= 1\n",
      "obs= [ 0.02440125  0.05484138 -0.00600539 -0.13765886] reward= 1.0 done= False info= {}\n",
      "step i 50 action= 0\n",
      "obs= [ 0.02549807 -0.14019405 -0.00875856  0.15312343] reward= 1.0 done= False info= {}\n",
      "step i 51 action= 1\n",
      "obs= [ 0.02269419  0.05505221 -0.0056961  -0.14230971] reward= 1.0 done= False info= {}\n",
      "step i 52 action= 0\n",
      "obs= [ 0.02379524 -0.1399877  -0.00854229  0.14857077] reward= 1.0 done= False info= {}\n",
      "step i 53 action= 1\n",
      "obs= [ 0.02099548  0.05525553 -0.00557088 -0.14679479] reward= 1.0 done= False info= {}\n",
      "step i 54 action= 0\n",
      "obs= [ 0.02210059 -0.1397862  -0.00850677  0.14412544] reward= 1.0 done= False info= {}\n",
      "step i 55 action= 1\n",
      "obs= [ 0.01930487  0.05545654 -0.00562426 -0.15122903] reward= 1.0 done= False info= {}\n",
      "step i 56 action= 0\n",
      "obs= [ 0.020414   -0.13958443 -0.00864884  0.13967426] reward= 1.0 done= False info= {}\n",
      "step i 57 action= 1\n",
      "obs= [ 0.01762231  0.05566032 -0.00585536 -0.15572463] reward= 1.0 done= False info= {}\n",
      "step i 58 action= 0\n",
      "obs= [ 0.01873552 -0.13937731 -0.00896985  0.13510533] reward= 1.0 done= False info= {}\n",
      "step i 59 action= 1\n",
      "obs= [ 0.01594797  0.05587197 -0.00626774 -0.1603939 ] reward= 1.0 done= False info= {}\n",
      "step i 60 action= 0\n",
      "obs= [ 0.01706541 -0.1391597  -0.00947562  0.13030514] reward= 1.0 done= False info= {}\n",
      "step i 61 action= 1\n",
      "obs= [ 0.01428222  0.0560967  -0.00686952 -0.1653521 ] reward= 1.0 done= False info= {}\n",
      "step i 62 action= 0\n",
      "obs= [ 0.01540415 -0.13892624 -0.01017656  0.12515579] reward= 1.0 done= False info= {}\n",
      "step i 63 action= 1\n",
      "obs= [ 0.01262563  0.05634001 -0.00767345 -0.17072028] reward= 1.0 done= False info= {}\n",
      "step i 64 action= 0\n",
      "obs= [ 0.01375243 -0.13867128 -0.01108785  0.11953207] reward= 1.0 done= False info= {}\n",
      "step i 65 action= 1\n",
      "obs= [ 0.010979    0.05660777 -0.00869721 -0.17662825] reward= 1.0 done= False info= {}\n",
      "step i 66 action= 0\n",
      "obs= [ 0.01211116 -0.13838865 -0.01222977  0.11329835] reward= 1.0 done= False info= {}\n",
      "step i 67 action= 1\n",
      "obs= [ 0.00934338  0.05690639 -0.00996381 -0.18321776] reward= 1.0 done= False info= {}\n",
      "step i 68 action= 0\n",
      "obs= [ 0.01048151 -0.13807158 -0.01362816  0.10630538] reward= 1.0 done= False info= {}\n",
      "step i 69 action= 1\n",
      "obs= [ 0.00772008  0.05724299 -0.01150206 -0.19064583] reward= 1.0 done= False info= {}\n",
      "step i 70 action= 0\n",
      "obs= [ 0.00886494 -0.13771254 -0.01531497  0.09838659] reward= 1.0 done= False info= {}\n",
      "step i 71 action= 1\n",
      "obs= [ 0.00611069  0.05762552 -0.01334724 -0.19908856] reward= 1.0 done= False info= {}\n",
      "step i 72 action= 0\n",
      "obs= [ 0.0072632  -0.13730301 -0.01732901  0.08935422] reward= 1.0 done= False info= {}\n",
      "step i 73 action= 1\n",
      "obs= [ 0.00451714  0.05806298 -0.01554193 -0.20874525] reward= 1.0 done= False info= {}\n",
      "step i 74 action= 0\n",
      "obs= [ 0.0056784  -0.13683333 -0.01971683  0.07899476] reward= 1.0 done= False info= {}\n",
      "step i 75 action= 1\n",
      "obs= [ 0.00294173  0.05856564 -0.01813694 -0.21984309] reward= 1.0 done= False info= {}\n",
      "step i 76 action= 0\n",
      "obs= [ 0.00411304 -0.13629242 -0.0225338   0.06706396] reward= 1.0 done= False info= {}\n",
      "step i 77 action= 1\n",
      "obs= [ 0.0013872   0.05914523 -0.02119252 -0.23264247] reward= 1.0 done= False info= {}\n",
      "step i 78 action= 0\n",
      "obs= [ 0.0025701  -0.1356676  -0.02584537  0.05328102] reward= 1.0 done= False info= {}\n",
      "step i 79 action= 1\n",
      "obs= [-1.43251111e-04  5.98152233e-02 -2.47797480e-02 -2.47442973e-01] reward= 1.0 done= False info= {}\n",
      "step i 80 action= 0\n",
      "obs= [ 0.00105305 -0.13494423 -0.02972861  0.03732201] reward= 1.0 done= False info= {}\n",
      "step i 81 action= 1\n",
      "obs= [-0.00164583  0.06059114 -0.02898217 -0.26459036] reward= 1.0 done= False info= {}\n",
      "step i 82 action= 0\n",
      "obs= [-0.00043401 -0.13410541 -0.03427397  0.01881226] reward= 1.0 done= False info= {}\n",
      "step i 83 action= 1\n",
      "obs= [-0.00311612  0.06149088 -0.03389773 -0.28448444] reward= 1.0 done= False info= {}\n",
      "step i 84 action= 0\n",
      "obs= [-0.0018863  -0.13313162 -0.03958742 -0.00268243] reward= 1.0 done= False info= {}\n",
      "step i 85 action= 0\n",
      "obs= [-0.00454893 -0.32766413 -0.03964107  0.27725206] reward= 1.0 done= False info= {}\n",
      "step i 86 action= 1\n",
      "obs= [-0.01110221 -0.13199972 -0.03409603 -0.02766541] reward= 1.0 done= False info= {}\n",
      "step i 87 action= 0\n",
      "obs= [-0.01374221 -0.32661654 -0.03464933  0.25406786] reward= 1.0 done= False info= {}\n",
      "step i 88 action= 1\n",
      "obs= [-0.02027454 -0.13101742 -0.02956798 -0.04933957] reward= 1.0 done= False info= {}\n",
      "step i 89 action= 0\n",
      "obs= [-0.02289489 -0.32570319 -0.03055477  0.23386972] reward= 1.0 done= False info= {}\n",
      "step i 90 action= 1\n",
      "obs= [-0.02940895 -0.1301583  -0.02587737 -0.06829242] reward= 1.0 done= False info= {}\n",
      "step i 91 action= 0\n",
      "obs= [-0.03201212 -0.32489987 -0.02724322  0.21611505] reward= 1.0 done= False info= {}\n",
      "step i 92 action= 1\n",
      "obs= [-0.03851011 -0.12939926 -0.02292092 -0.08503557] reward= 1.0 done= False info= {}\n",
      "step i 93 action= 0\n",
      "obs= [-0.0410981  -0.32418529 -0.02462163  0.20032856] reward= 1.0 done= False info= {}\n",
      "step i 94 action= 1\n",
      "obs= [-0.04758181 -0.12872    -0.02061506 -0.10001853] reward= 1.0 done= False info= {}\n",
      "step i 95 action= 0\n",
      "obs= [-0.05015621 -0.32354053 -0.02261543  0.18608977] reward= 1.0 done= False info= {}\n",
      "step i 96 action= 1\n",
      "obs= [-0.05662702 -0.12810243 -0.01889364 -0.11364079] reward= 1.0 done= False info= {}\n",
      "step i 97 action= 0\n",
      "obs= [-0.05918906 -0.32294863 -0.02116645  0.17302195] reward= 1.0 done= False info= {}\n",
      "step i 98 action= 1\n",
      "obs= [-0.06564804 -0.12753023 -0.01770601 -0.12626245] reward= 1.0 done= False info= {}\n",
      "step i 99 action= 0\n",
      "obs= [-0.06819864 -0.32239411 -0.02023126  0.16078225] reward= 1.0 done= False info= {}\n",
      "step i 100 action= 1\n",
      "obs= [-0.07464652 -0.12698846 -0.01701562 -0.13821383] reward= 1.0 done= False info= {}\n",
      "step i 101 action= 0\n",
      "obs= [-0.07718629 -0.32186262 -0.01977989  0.14905274] reward= 1.0 done= False info= {}\n",
      "step i 102 action= 1\n",
      "obs= [-0.08362355 -0.12646309 -0.01679884 -0.14980413] reward= 1.0 done= False info= {}\n",
      "step i 103 action= 0\n",
      "obs= [-0.08615281 -0.32134051 -0.01979492  0.13753213] reward= 1.0 done= False info= {}\n",
      "step i 104 action= 1\n",
      "obs= [-0.09257962 -0.12594071 -0.01704428 -0.16132947] reward= 1.0 done= False info= {}\n",
      "step i 105 action= 0\n",
      "obs= [-0.09509843 -0.32081456 -0.02027087  0.12592807] reward= 1.0 done= False info= {}\n",
      "step i 106 action= 1\n",
      "obs= [-0.10151472 -0.12540816 -0.01775231 -0.17308054] reward= 1.0 done= False info= {}\n",
      "step i 107 action= 0\n",
      "obs= [-0.10402289 -0.3202716  -0.02121392  0.11394966] reward= 1.0 done= False info= {}\n",
      "step i 108 action= 1\n",
      "obs= [-0.11042832 -0.1248522  -0.01893492 -0.1853499 ] reward= 1.0 done= False info= {}\n",
      "step i 109 action= 0\n",
      "obs= [-0.11292536 -0.31969819 -0.02264192  0.10130015] reward= 1.0 done= False info= {}\n",
      "step i 110 action= 1\n",
      "obs= [-0.11931933 -0.12425919 -0.02061592 -0.19843937] reward= 1.0 done= False info= {}\n",
      "step i 111 action= 0\n",
      "obs= [-0.12180451 -0.31908029 -0.02458471  0.08766954] reward= 1.0 done= False info= {}\n",
      "step i 112 action= 1\n",
      "obs= [-0.12818612 -0.12361473 -0.02283132 -0.21266738] reward= 1.0 done= False info= {}\n",
      "step i 113 action= 0\n",
      "obs= [-0.13065841 -0.31840294 -0.02708466  0.07272696] reward= 1.0 done= False info= {}\n",
      "step i 114 action= 1\n",
      "obs= [-0.13702647 -0.12290337 -0.02563013 -0.22837675] reward= 1.0 done= False info= {}\n",
      "step i 115 action= 0\n",
      "obs= [-0.13948454 -0.31764986 -0.03019766  0.0561126 ] reward= 1.0 done= False info= {}\n",
      "step i 116 action= 1\n",
      "obs= [-0.14583753 -0.12210823 -0.02907541 -0.24594288] reward= 1.0 done= False info= {}\n",
      "step i 117 action= 0\n",
      "obs= [-0.1482797  -0.31680311 -0.03399427  0.03742904] reward= 1.0 done= False info= {}\n",
      "step i 118 action= 1\n",
      "obs= [-0.15461576 -0.12121058 -0.03324568 -0.26578263] reward= 1.0 done= False info= {}\n",
      "step i 119 action= 0\n",
      "obs= [-0.15703997 -0.31584265 -0.03856134  0.01623171] reward= 1.0 done= False info= {}\n",
      "step i 120 action= 1\n",
      "obs= [-0.16335683 -0.12018951 -0.0382367  -0.28836406] reward= 1.0 done= False info= {}\n",
      "step i 121 action= 0\n",
      "obs= [-0.16576062 -0.31474592 -0.04400398 -0.0079816 ] reward= 1.0 done= False info= {}\n",
      "step i 122 action= 0\n",
      "obs= [-0.17205553 -0.50921007 -0.04416362  0.27049926] reward= 1.0 done= False info= {}\n",
      "step i 123 action= 1\n",
      "obs= [-0.18223973 -0.31348664 -0.03875363 -0.03577951] reward= 1.0 done= False info= {}\n",
      "step i 124 action= 0\n",
      "obs= [-0.18850947 -0.50803204 -0.03946922  0.24442878] reward= 1.0 done= False info= {}\n",
      "step i 125 action= 1\n",
      "obs= [-0.19867011 -0.31236922 -0.03458065 -0.06043781] reward= 1.0 done= False info= {}\n",
      "step i 126 action= 0\n",
      "obs= [-0.20491749 -0.50697874 -0.0357894   0.22113725] reward= 1.0 done= False info= {}\n",
      "step i 127 action= 1\n",
      "obs= [-0.21505707 -0.31136398 -0.03136666 -0.08261682] reward= 1.0 done= False info= {}\n",
      "step i 128 action= 0\n",
      "obs= [-0.22128435 -0.50602259 -0.03301899  0.20000717] reward= 1.0 done= False info= {}\n",
      "step i 129 action= 1\n",
      "obs= [-0.2314048  -0.31044432 -0.02901885 -0.10290613] reward= 1.0 done= False info= {}\n",
      "step i 130 action= 0\n",
      "obs= [-0.23761369 -0.50513864 -0.03107697  0.18048204] reward= 1.0 done= False info= {}\n",
      "step i 131 action= 1\n",
      "obs= [-0.24771646 -0.30958609 -0.02746733 -0.12184031] reward= 1.0 done= False info= {}\n",
      "step i 132 action= 0\n",
      "obs= [-0.25390818 -0.50430397 -0.02990414  0.16205196] reward= 1.0 done= False info= {}\n",
      "step i 133 action= 1\n",
      "obs= [-0.26399426 -0.30876696 -0.0266631  -0.13991293] reward= 1.0 done= False info= {}\n",
      "step i 134 action= 0\n",
      "obs= [-0.2701696  -0.50349709 -0.02946136  0.14424042] reward= 1.0 done= False info= {}\n",
      "step i 135 action= 1\n",
      "obs= [-0.28023954 -0.30796587 -0.02657655 -0.15758951] reward= 1.0 done= False info= {}\n",
      "step i 136 action= 0\n",
      "obs= [-0.28639886 -0.50269744 -0.02972834  0.12659211] reward= 1.0 done= False info= {}\n",
      "step i 137 action= 1\n",
      "obs= [-0.29645281 -0.3071625  -0.0271965  -0.17531956] reward= 1.0 done= False info= {}\n",
      "step i 138 action= 0\n",
      "obs= [-0.30259606 -0.50188488 -0.03070289  0.10866122] reward= 1.0 done= False info= {}\n",
      "step i 139 action= 1\n",
      "obs= [-0.31263375 -0.30633672 -0.02852966 -0.19354801] reward= 1.0 done= False info= {}\n",
      "step i 140 action= 0\n",
      "obs= [-0.31876049 -0.50103919 -0.03240062  0.09000026] reward= 1.0 done= False info= {}\n",
      "step i 141 action= 1\n",
      "obs= [-0.32878127 -0.30546816 -0.03060062 -0.21272643] reward= 1.0 done= False info= {}\n",
      "step i 142 action= 0\n",
      "obs= [-0.33489064 -0.50013955 -0.03485515  0.07014877] reward= 1.0 done= False info= {}\n",
      "step i 143 action= 1\n",
      "obs= [-0.34489343 -0.30453567 -0.03345217 -0.23332417] reward= 1.0 done= False info= {}\n",
      "step i 144 action= 0\n",
      "obs= [-0.35098414 -0.49916408 -0.03811866  0.04862199] reward= 1.0 done= False info= {}\n",
      "step i 145 action= 1\n",
      "obs= [-0.36096742 -0.30351686 -0.03714622 -0.25583982] reward= 1.0 done= False info= {}\n",
      "step i 146 action= 0\n",
      "obs= [-0.36703776 -0.4980893  -0.04226301  0.02489903] reward= 1.0 done= False info= {}\n",
      "step i 147 action= 1\n",
      "obs= [-0.37699955 -0.30238755 -0.04176503 -0.2808131 ] reward= 1.0 done= False info= {}\n",
      "step i 148 action= 0\n",
      "obs= [-0.3830473  -0.49688962 -0.04738129 -0.00158975] reward= 1.0 done= False info= {}\n",
      "step i 149 action= 0\n",
      "obs= [-0.39298509 -0.69130116 -0.04741309  0.27577539] reward= 1.0 done= False info= {}\n",
      "step i 150 action= 1\n",
      "obs= [-0.40681111 -0.49553595 -0.04189758 -0.03147665] reward= 1.0 done= False info= {}\n",
      "step i 151 action= 0\n",
      "obs= [-0.41672183 -0.69003281 -0.04252711  0.24769831] reward= 1.0 done= False info= {}\n",
      "step i 152 action= 1\n",
      "obs= [-0.43052249 -0.49433012 -0.03757315 -0.05808951] reward= 1.0 done= False info= {}\n",
      "step i 153 action= 0\n",
      "obs= [-0.44040909 -0.68889376 -0.03873494  0.22250606] reward= 1.0 done= False info= {}\n",
      "step i 154 action= 1\n",
      "obs= [-0.45418696 -0.49324018 -0.03428482 -0.08213938] reward= 1.0 done= False info= {}\n",
      "step i 155 action= 0\n",
      "obs= [-0.46405177 -0.68785432 -0.0359276   0.19953251] reward= 1.0 done= False info= {}\n",
      "step i 156 action= 1\n",
      "obs= [-0.47780885 -0.49223742 -0.03193695 -0.10426393] reward= 1.0 done= False info= {}\n",
      "step i 157 action= 0\n",
      "obs= [-0.4876536  -0.68688748 -0.03402223  0.1781744 ] reward= 1.0 done= False info= {}\n",
      "step i 158 action= 1\n",
      "obs= [-0.50139135 -0.49129559 -0.03045874 -0.12504426] reward= 1.0 done= False info= {}\n",
      "step i 159 action= 0\n",
      "obs= [-0.51121726 -0.68596824 -0.03295963  0.15787579] reward= 1.0 done= False info= {}\n",
      "step i 160 action= 1\n",
      "obs= [-0.52493663 -0.49039028 -0.02980211 -0.14502019] reward= 1.0 done= False info= {}\n",
      "step i 161 action= 0\n",
      "obs= [-0.53474443 -0.68507304 -0.03270252  0.13811376] reward= 1.0 done= False info= {}\n",
      "step i 162 action= 1\n",
      "obs= [-0.5484459  -0.48949833 -0.02994024 -0.16470428] reward= 1.0 done= False info= {}\n",
      "step i 163 action= 0\n",
      "obs= [-0.55823586 -0.68417918 -0.03323433  0.118385  ] reward= 1.0 done= False info= {}\n",
      "step i 164 action= 1\n",
      "obs= [-0.57191945 -0.4885972  -0.03086663 -0.18459506] reward= 1.0 done= False info= {}\n",
      "step i 165 action= 0\n",
      "obs= [-0.58169139 -0.68326422 -0.03455853  0.09819303] reward= 1.0 done= False info= {}\n",
      "step i 166 action= 1\n",
      "obs= [-0.59535667 -0.48766446 -0.03259467 -0.20518961] reward= 1.0 done= False info= {}\n",
      "step i 167 action= 0\n",
      "obs= [-0.60510996 -0.6823055  -0.03669846  0.07703573] reward= 1.0 done= False info= {}\n",
      "step i 168 action= 1\n",
      "obs= [-0.61875607 -0.48667719 -0.03515775 -0.22699608] reward= 1.0 done= False info= {}\n",
      "step i 169 action= 0\n",
      "obs= [-0.62848962 -0.68127952 -0.03969767  0.05439278] reward= 1.0 done= False info= {}\n",
      "step i 170 action= 1\n",
      "obs= [-0.64211521 -0.48561151 -0.03860981 -0.25054605] reward= 1.0 done= False info= {}\n",
      "step i 171 action= 0\n",
      "obs= [-0.65182744 -0.68016144 -0.04362073  0.0297129 ] reward= 1.0 done= False info= {}\n",
      "step i 172 action= 1\n",
      "obs= [-0.66543067 -0.48444197 -0.04302648 -0.27640748] reward= 1.0 done= False info= {}\n",
      "step i 173 action= 0\n",
      "obs= [-0.67511951 -0.6789245  -0.04855462  0.00240044] reward= 1.0 done= False info= {}\n",
      "step i 174 action= 1\n",
      "obs= [-0.688698   -0.48314104 -0.04850662 -0.30519808] reward= 1.0 done= False info= {}\n",
      "step i 175 action= 0\n",
      "obs= [-0.69836082 -0.67753939 -0.05461058 -0.02819889] reward= 1.0 done= False info= {}\n",
      "step i 176 action= 0\n",
      "obs= [-0.7119116  -0.87183739 -0.05517456  0.24676609] reward= 1.0 done= False info= {}\n",
      "step i 177 action= 1\n",
      "obs= [-0.72934835 -0.67597262 -0.05023923 -0.06279708] reward= 1.0 done= False info= {}\n",
      "step i 178 action= 0\n",
      "obs= [-0.7428678  -0.87033961 -0.05149518  0.21362142] reward= 1.0 done= False info= {}\n",
      "step i 179 action= 1\n",
      "obs= [-0.7602746  -0.6745207  -0.04722275 -0.09485047] reward= 1.0 done= False info= {}\n",
      "step i 180 action= 0\n",
      "obs= [-0.77376501 -0.86893513 -0.04911976  0.18256775] reward= 1.0 done= False info= {}\n",
      "step i 181 action= 1\n",
      "obs= [-0.79114371 -0.67314599 -0.0454684  -0.12519713] reward= 1.0 done= False info= {}\n",
      "step i 182 action= 0\n",
      "obs= [-0.80460663 -0.86758806 -0.04797234  0.15280143] reward= 1.0 done= False info= {}\n",
      "step i 183 action= 1\n",
      "obs= [-0.82195839 -0.6718132  -0.04491631 -0.15462147] reward= 1.0 done= False info= {}\n",
      "step i 184 action= 0\n",
      "obs= [-0.83539466 -0.86626422 -0.04800874  0.12355992] reward= 1.0 done= False info= {}\n",
      "step i 185 action= 1\n",
      "obs= [-0.85271994 -0.67048851 -0.04553755 -0.18387443] reward= 1.0 done= False info= {}\n",
      "step i 186 action= 0\n",
      "obs= [-0.86612971 -0.86493031 -0.04921503  0.0941025 ] reward= 1.0 done= False info= {}\n",
      "step i 187 action= 1\n",
      "obs= [-0.88342832 -0.66913878 -0.04733298 -0.21369257] reward= 1.0 done= False info= {}\n",
      "step i 188 action= 0\n",
      "obs= [-0.89681109 -0.86355318 -0.05160684  0.06369171] reward= 1.0 done= False info= {}\n",
      "step i 189 action= 1\n",
      "obs= [-0.91408216 -0.66773076 -0.050333   -0.24481654] reward= 1.0 done= False info= {}\n",
      "step i 190 action= 0\n",
      "obs= [-0.92743677 -0.86209902 -0.05522933  0.03157497] reward= 1.0 done= False info= {}\n",
      "step i 191 action= 1\n",
      "obs= [-0.94467875 -0.66623033 -0.05459783 -0.27800926] reward= 1.0 done= False info= {}\n",
      "step i 192 action= 0\n",
      "obs= [-0.95800336 -0.8605326  -0.06015802 -0.00303393] reward= 1.0 done= False info= {}\n",
      "step i 193 action= 0\n",
      "obs= [-0.97521401 -1.05474252 -0.0602187   0.27007817] reward= 1.0 done= False info= {}\n",
      "step i 194 action= 1\n",
      "obs= [-0.99630886 -0.85881523 -0.05481713 -0.04097364] reward= 1.0 done= False info= {}\n",
      "step i 195 action= 0\n",
      "obs= [-1.01348517 -1.05311001 -0.05563661  0.23392259] reward= 1.0 done= False info= {}\n",
      "step i 196 action= 1\n",
      "obs= [-1.03454737 -0.85723909 -0.05095815 -0.07577792] reward= 1.0 done= False info= {}\n",
      "step i 197 action= 0\n",
      "obs= [-1.05169215 -1.05159487 -0.05247371  0.20040212] reward= 1.0 done= False info= {}\n",
      "step i 198 action= 1\n",
      "obs= [-1.07272405 -0.85576323 -0.04846567 -0.10836122] reward= 1.0 done= False info= {}\n",
      "step i 199 action= 0\n",
      "obs= [-1.08983931 -1.05015837 -0.0506329   0.16864576] reward= 1.0 done= True info= {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "#test \n",
    "A = env.action_space.n\n",
    "# Test\n",
    "done = False\n",
    "obs = env.reset()\n",
    "for i in range(50000):\n",
    "    state = discretize_state(obs)\n",
    "    action = pi[state]\n",
    "\n",
    "    print(\"step i\",i,\"action=\",action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
