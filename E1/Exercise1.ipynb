{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdlBJO75Ww_S"
   },
   "source": [
    "## A. Read Chapter 3 and do the following exercises: 3.7-3.9, 3.12, 3.18, 3.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmwycwvsWr_x"
   },
   "source": [
    ">**3.7** Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n",
    "\n",
    "The time of escaping the maze is not counted in the reward. Since if the robot escape from the maze, no matter how much time it spent, the reward is always 1. You can give a reward of 0 for escaping and a reward of -1 at all other times.\n",
    ">**3.8** Suppose $\\gamma$ = 0.5 and the following sequence of rewards is received R1 = −1, R2 = 2, R3 = 6, R4 = 3, and R5 = 2, with T = 5. What are G0, G1,..., G5? Hint: Work backwards.\n",
    "\n",
    "Based on equation (3.9), $G_{t} = R_{t+1} + \\gamma G_{t+1}$. So:\n",
    "$$ \\begin{align*}\n",
    "G_5 &= 0 \\\\\n",
    "G_4 &= R_5 + \\gamma G_5 = 2 \\\\\n",
    "G_3 &= R_4 + \\gamma G_4 = 4 \\\\\n",
    "G_2 &= R_3 + \\gamma G_3 = 8 \\\\\n",
    "G_1 &= R_2 + \\gamma G_2 = 6 \\\\\n",
    "G_0 &= R_1 + \\gamma G_1 = 2 \n",
    "\\end{align*}$$\n",
    "\n",
    ">**3.9** Suppose $\\gamma$ = 0.9 and the reward sequence is R1 = 2 followed by an infinite sequence of 7s. What are G1 and G0?\n",
    "\n",
    "R1 = 2, R2 = 7, R3 = 7,.... Based on equation (3.8):\n",
    "$$ \n",
    "\\begin{align*}\n",
    "G_1 &\\doteq \\sum_{k=0}^\\infty \\gamma^k R_{1+k+1} = \\sum_{k=0}^\\infty 7\\gamma^k =7 \\sum_{k=0}^\\infty 0.9^k = \\frac{7}{1-0.9} = 70\n",
    "\\\\\n",
    "\\\\\n",
    "G_0 &= R_1 + \\gamma G_1 = 2 + 0.9\\times70 = 65\n",
    "\\end{align*}\n",
    "$$\n",
    ">**3.12** Give an equation for $v_{\\pi}$ in terms of $q_{\\pi}$ and $\\pi$.\n",
    "$$ \\begin{align*}\n",
    "v_{\\pi}(s) &\\doteq \\mathbb{E}_{\\pi}\n",
    "\\left[ G_t | S_t=s \\right] = \\sum_{a\\in \\mathcal{A}} \\pi(a|s) \\mathbb{E}_{\\pi}\n",
    "\\left[ G_t | S_t=s, A_t=a \\right]\n",
    "\\\\\n",
    "&= \\sum_{a\\in \\mathcal{A}} \\pi(a|s) q_{\\pi} (s,a) \n",
    "\\end{align*} $$\n",
    "\n",
    ">**3.18** \n",
    "\n",
    "$$ \\begin{align*}\n",
    "v_{\\pi}&= \\sum_{a\\in \\mathcal{A}} \\pi(a|s) q_{\\pi} (s,a) \n",
    "\\end{align*} $$\n",
    ">**3.19**\n",
    "\n",
    "$$ \\begin{align*}\n",
    "q_{\\pi}(s,a) &= \\mathbb{E}_{p(s',r|s,a)}\\left[\n",
    "r+v_{\\pi}(s')\n",
    "| S_t=s,A_t=a\n",
    "\\right]\n",
    "\\\\\n",
    "&= \\sum_{s'\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}\n",
    "    p(s',r|s,a)(r+v_{\\pi}(s'))\n",
    "\\end{align*} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LviqmwNfTDbR"
   },
   "source": [
    "##B. Practical\n",
    "### 1. Anatomy\n",
    "\n",
    "> Read the code and observe how the concepts of the course thus far relate to it. Comment on these correspondences. What does an RL agent solve? What should a RL agent have inside of it? Do you observe these in the program?\n",
    "\n",
    "The RL agent solves the maximization of the action value function and tries to win the TTT game. The agent should generate possible actions, have an action selection policy, a knowledge tuning system to receive feedback from the environment. In this system, the agent generates the possible moves on the board, and the policy choose either exploit or explore by `if np.random.rand() < self.epsilon`, for exploit it selects action by sorting and return the action that maximizes the value function, and the action of the last episode is used as `backup()` function to solve the issue of the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "bvRyWpdSWqIz",
    "outputId": "f4aff4cc-e6b4-421c-dffb-29be99efea48"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"if __name__ == '__main__':\\n    train(int(100))\\n    compete(int(10))\\n    #play()\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 - 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)           #\n",
    "# 2016 Jan Hakenberg(jan.hakenberg@gmail.com)                         #\n",
    "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3\n",
    "BOARD_SIZE = BOARD_ROWS * BOARD_COLS\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self):\n",
    "        # the board is represented by an n * n array,\n",
    "        # 1 represents a chessman of the player who moves first,\n",
    "        # -1 represents a chessman of another player\n",
    "        # 0 represents an empty position\n",
    "        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.winner = None\n",
    "        self.hash_val = None\n",
    "        self.end = None\n",
    "\n",
    "    # compute the hash value for one state, it's unique\n",
    "    def hash(self):\n",
    "        if self.hash_val is None:\n",
    "            self.hash_val = 0\n",
    "            for i in np.nditer(self.data):\n",
    "                self.hash_val = self.hash_val * 3 + i + 1\n",
    "        return self.hash_val\n",
    "\n",
    "    # check whether a player has won the game, or it's a tie\n",
    "    def is_end(self):\n",
    "        if self.end is not None:\n",
    "            return self.end\n",
    "        results = []\n",
    "        # check row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            results.append(np.sum(self.data[i, :]))\n",
    "        # check columns\n",
    "        for i in range(BOARD_COLS):\n",
    "            results.append(np.sum(self.data[:, i]))\n",
    "\n",
    "        # check diagonals\n",
    "        trace = 0\n",
    "        reverse_trace = 0\n",
    "        for i in range(BOARD_ROWS):\n",
    "            trace += self.data[i, i]\n",
    "            reverse_trace += self.data[i, BOARD_ROWS - 1 - i]\n",
    "        results.append(trace)\n",
    "        results.append(reverse_trace)\n",
    "\n",
    "        for result in results:\n",
    "            if result == 3:\n",
    "                self.winner = 1\n",
    "                self.end = True\n",
    "                return self.end\n",
    "            if result == -3:\n",
    "                self.winner = -1\n",
    "                self.end = True\n",
    "                return self.end\n",
    "\n",
    "        # whether it's a tie\n",
    "        sum_values = np.sum(np.abs(self.data))\n",
    "        if sum_values == BOARD_SIZE:\n",
    "            self.winner = 0\n",
    "            self.end = True\n",
    "            return self.end\n",
    "\n",
    "        # game is still going on\n",
    "        self.end = False\n",
    "        return self.end\n",
    "\n",
    "    # @symbol: 1 or -1\n",
    "    # put chessman symbol in position (i, j)\n",
    "    def next_state(self, i, j, symbol):\n",
    "        new_state = State()\n",
    "        new_state.data = np.copy(self.data)\n",
    "        new_state.data[i, j] = symbol\n",
    "        return new_state\n",
    "\n",
    "    # print the board\n",
    "    def print_state(self):\n",
    "        for i in range(BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.data[i, j] == 1:\n",
    "                    token = '*'\n",
    "                elif self.data[i, j] == -1:\n",
    "                    token = 'x'\n",
    "                else:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')\n",
    "\n",
    "\n",
    "def get_all_states_impl(current_state, current_symbol, all_states):\n",
    "    for i in range(BOARD_ROWS):\n",
    "        for j in range(BOARD_COLS):\n",
    "            if current_state.data[i][j] == 0:\n",
    "                new_state = current_state.next_state(i, j, current_symbol)\n",
    "                new_hash = new_state.hash()\n",
    "                if new_hash not in all_states:\n",
    "                    is_end = new_state.is_end()\n",
    "                    all_states[new_hash] = (new_state, is_end)\n",
    "                    if not is_end:\n",
    "                        get_all_states_impl(new_state, -current_symbol, all_states)\n",
    "\n",
    "\n",
    "def get_all_states():\n",
    "    current_symbol = 1\n",
    "    current_state = State()\n",
    "    all_states = dict()\n",
    "    all_states[current_state.hash()] = (current_state, current_state.is_end())\n",
    "    get_all_states_impl(current_state, current_symbol, all_states)\n",
    "    return all_states\n",
    "\n",
    "\n",
    "# all possible board configurations\n",
    "all_states = get_all_states()\n",
    "\n",
    "\n",
    "class Judger:\n",
    "    # @player1: the player who will move first, its chessman will be 1\n",
    "    # @player2: another player with a chessman -1\n",
    "    def __init__(self, player1, player2):\n",
    "        self.p1 = player1\n",
    "        self.p2 = player2\n",
    "        self.current_player = None\n",
    "        self.p1_symbol = 1\n",
    "        self.p2_symbol = -1\n",
    "        self.p1.set_symbol(self.p1_symbol)\n",
    "        self.p2.set_symbol(self.p2_symbol)\n",
    "        self.current_state = State()\n",
    "\n",
    "    def reset(self):\n",
    "        self.p1.reset()\n",
    "        self.p2.reset()\n",
    "\n",
    "    def alternate(self):\n",
    "        while True:\n",
    "            yield self.p1\n",
    "            yield self.p2\n",
    "\n",
    "    # @print_state: if True, print each board during the game\n",
    "    def play(self, print_state=False):\n",
    "        alternator = self.alternate()\n",
    "        self.reset()\n",
    "        current_state = State()\n",
    "        self.p1.set_state(current_state)\n",
    "        self.p2.set_state(current_state)\n",
    "        if print_state:\n",
    "            current_state.print_state()\n",
    "        while True:\n",
    "            player = next(alternator)\n",
    "            i, j, symbol = player.act()\n",
    "            next_state_hash = current_state.next_state(i, j, symbol).hash()\n",
    "            current_state, is_end = all_states[next_state_hash]\n",
    "            self.p1.set_state(current_state)\n",
    "            self.p2.set_state(current_state)\n",
    "            if print_state:\n",
    "                current_state.print_state()\n",
    "            if is_end:\n",
    "                return current_state.winner\n",
    "\n",
    "\n",
    "# AI player\n",
    "class Player:\n",
    "    # @step_size: the step size to update estimations\n",
    "    # @epsilon: the probability to explore\n",
    "    def __init__(self, step_size=0.1, epsilon=0.1):\n",
    "        self.estimations = dict()\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.states = []\n",
    "        self.greedy = []\n",
    "        self.symbol = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.greedy = []\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.states.append(state)\n",
    "        self.greedy.append(True)\n",
    "\n",
    "    def set_symbol(self, symbol):\n",
    "        self.symbol = symbol\n",
    "        for hash_val in all_states:\n",
    "            state, is_end = all_states[hash_val]\n",
    "            if is_end:\n",
    "                if state.winner == self.symbol:\n",
    "                    self.estimations[hash_val] = 1.0\n",
    "                elif state.winner == 0:\n",
    "                    # we need to distinguish between a tie and a lose\n",
    "                    self.estimations[hash_val] = 0.5\n",
    "                else:\n",
    "                    self.estimations[hash_val] = 0\n",
    "            else:\n",
    "                self.estimations[hash_val] = 0.5\n",
    "\n",
    "    # update value estimation\n",
    "    def backup(self):\n",
    "        states = [state.hash() for state in self.states]\n",
    "\n",
    "        for i in reversed(range(len(states) - 1)):\n",
    "            state = states[i]\n",
    "            td_error = self.greedy[i] * (\n",
    "                self.estimations[states[i + 1]] - self.estimations[state]\n",
    "            )\n",
    "            self.estimations[state] += self.step_size * td_error\n",
    "\n",
    "    # choose an action based on the state\n",
    "    def act(self):\n",
    "        state = self.states[-1]\n",
    "        next_states = []\n",
    "        next_positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if state.data[i, j] == 0:\n",
    "                    next_positions.append([i, j])\n",
    "                    next_states.append(state.next_state(\n",
    "                        i, j, self.symbol).hash())\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            print(\"explore\")\n",
    "            action = next_positions[np.random.randint(len(next_positions))]\n",
    "            action.append(self.symbol)\n",
    "            self.greedy[-1] = False\n",
    "            return action\n",
    "\n",
    "        print(\"exploit\")\n",
    "        values = []\n",
    "        for hash_val, pos in zip(next_states, next_positions):\n",
    "            values.append((self.estimations[hash_val], pos))\n",
    "        # to select one of the actions of equal value at random due to Python's sort is stable\n",
    "        np.random.shuffle(values)\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "        action = values[0][1]\n",
    "        print(\"value function:\",values[0][0])\n",
    "        action.append(self.symbol)\n",
    "        return action\n",
    "\n",
    "    def save_policy(self):\n",
    "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n",
    "            pickle.dump(self.estimations, f)\n",
    "\n",
    "    def load_policy(self):\n",
    "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n",
    "            self.estimations = pickle.load(f)\n",
    "\n",
    "\n",
    "# human interface\n",
    "# input a number to put a chessman\n",
    "# | q | w | e |\n",
    "# | a | s | d |\n",
    "# | z | x | c |\n",
    "class HumanPlayer:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.symbol = None\n",
    "        self.keys = ['q', 'w', 'e', 'a', 's', 'd', 'z', 'x', 'c']\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "\n",
    "    def set_symbol(self, symbol):\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def act(self):\n",
    "        self.state.print_state()\n",
    "        key = input(\"Input your position:\")\n",
    "        data = self.keys.index(key)\n",
    "        i = data // BOARD_COLS\n",
    "        j = data % BOARD_COLS\n",
    "        return i, j, self.symbol\n",
    "\n",
    "\n",
    "def train(epochs, print_every_n=10000):\n",
    "    player1 = Player(epsilon=0.01)\n",
    "    player2 = Player(epsilon=0.01)\n",
    "    judger = Judger(player1, player2)\n",
    "    player1_win = 0.0\n",
    "    player2_win = 0.0\n",
    "    for i in range(1, epochs + 1):\n",
    "        winner = judger.play(print_state=False)\n",
    "        if winner == 1:\n",
    "            player1_win += 1\n",
    "        if winner == -1:\n",
    "            player2_win += 1\n",
    "        if i % print_every_n == 0:\n",
    "            print('Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f' % (i, player1_win / i, player2_win / i))\n",
    "        player1.backup()\n",
    "        player2.backup()\n",
    "        judger.reset()\n",
    "    player1.save_policy()\n",
    "    player2.save_policy()\n",
    "\n",
    "\n",
    "def compete(turns):\n",
    "    player1 = Player(epsilon=0)\n",
    "    player2 = Player(epsilon=0)\n",
    "    judger = Judger(player1, player2)\n",
    "    player1.load_policy()\n",
    "    player2.load_policy()\n",
    "    player1_win = 0.0\n",
    "    player2_win = 0.0\n",
    "    for _ in range(turns):\n",
    "        winner = judger.play()\n",
    "        if winner == 1:\n",
    "            player1_win += 1\n",
    "        if winner == -1:\n",
    "            player2_win += 1\n",
    "        judger.reset()\n",
    "    print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))\n",
    "\n",
    "\n",
    "# The game is a zero sum game. If both players are playing with an optimal strategy, every game will end in a tie.\n",
    "# So we test whether the AI can guarantee at least a tie if it goes second.\n",
    "def play():\n",
    "    while True:\n",
    "        player1 = HumanPlayer()\n",
    "        player2 = Player(epsilon=0)\n",
    "        judger = Judger(player1, player2)\n",
    "        player2.load_policy()\n",
    "        winner = judger.play()\n",
    "        if winner == player2.symbol:\n",
    "            print(\"You lose!\")\n",
    "        elif winner == player1.symbol:\n",
    "            print(\"You win!\")\n",
    "        else:\n",
    "            print(\"It is a tie!\")\n",
    "\n",
    "\n",
    "'''if __name__ == '__main__':\n",
    "    train(int(100))\n",
    "    compete(int(10))\n",
    "    #play()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSlUoCxfOmtk"
   },
   "source": [
    "###2. Instrumentation and Play\n",
    ">Modify the code so it trains against you interactively. Instrument the code to print the value function as it learns, and also when it takes an exploit vs explore action. Observe how the agent takes exploitative actions and explorative actions, and how the value function changes.\n",
    "\n",
    ">Let the agent train against you for N games. Comment on how the agent's competence increases as N increases.\n",
    "\n",
    "With small N, the improvement of the agent is not obvious, because the agent has poor knowledge about the environment. However, the agent has learned to avoid repeating the move to loss when the same state occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wiqbkdZ6XjLS",
    "outputId": "11b1b3d1-e721-4811-e78f-91a919e87949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| 0 | x | 0 | \n",
      "-------------\n",
      "Input your position:c\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | 0 | x | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| 0 | x | * | \n",
      "-------------\n",
      "Input your position:q\n",
      "Epoch 1, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| x | 0 | 0 | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:w\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| x | * | x | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:x\n",
      "Epoch 2, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| x | * | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:q\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| * | 0 | 0 | \n",
      "-------------\n",
      "| x | * | 0 | \n",
      "-------------\n",
      "| x | 0 | 0 | \n",
      "-------------\n",
      "Input your position:c\n",
      "Epoch 3, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | x | 0 | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:q\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| * | x | 0 | \n",
      "-------------\n",
      "| 0 | * | x | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:c\n",
      "Epoch 4, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| 0 | 0 | x | \n",
      "-------------\n",
      "Input your position:x\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| x | * | 0 | \n",
      "-------------\n",
      "| 0 | * | x | \n",
      "-------------\n",
      "Input your position:w\n",
      "Epoch 5, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | * | x | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:q\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| * | 0 | 0 | \n",
      "-------------\n",
      "| 0 | * | x | \n",
      "-------------\n",
      "| x | 0 | 0 | \n",
      "-------------\n",
      "Input your position:c\n",
      "Epoch 6, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| x | 0 | 0 | \n",
      "-------------\n",
      "Input your position:x\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | 0 | x | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| x | * | 0 | \n",
      "-------------\n",
      "Input your position:w\n",
      "Epoch 7, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | 0 | x | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:w\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | * | x | \n",
      "-------------\n",
      "| x | * | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:x\n",
      "Epoch 8, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.4995\n",
      "-------------\n",
      "| 0 | 0 | x | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:w\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| 0 | * | x | \n",
      "-------------\n",
      "| 0 | * | 0 | \n",
      "-------------\n",
      "| 0 | 0 | x | \n",
      "-------------\n",
      "Input your position:x\n",
      "Epoch 9, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:s\n",
      "exploit\n",
      "value function: 0.4995\n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "| 0 | * | x | \n",
      "-------------\n",
      "| 0 | 0 | 0 | \n",
      "-------------\n",
      "Input your position:q\n",
      "exploit\n",
      "value function: 0.5\n",
      "-------------\n",
      "| * | 0 | 0 | \n",
      "-------------\n",
      "| 0 | * | x | \n",
      "-------------\n",
      "| 0 | x | 0 | \n",
      "-------------\n",
      "Input your position:c\n",
      "Epoch 10, player 1 winrate: 1.00, player 2 winrate: 0.00\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#%% Agent training against human\n",
    "def ai_against_human(epochs, print_every_n=1):\n",
    "    player1 = HumanPlayer()\n",
    "    player2 = Player(epsilon=0.01)\n",
    "\n",
    "    with open(\"policy_human_train.bin\", 'rb') as f:\n",
    "        player2.estimations = pickle.load(f)\n",
    " \n",
    "    judger = Judger(player1, player2)\n",
    "    player1_win = 0.0\n",
    "    player2_win = 0.0\n",
    "    player2.load_policy()\n",
    "    for i in range(1, epochs + 1):\n",
    "        winner = judger.play(print_state=False)\n",
    "        if winner == 1:\n",
    "            player1_win += 1\n",
    "        if winner == -1:\n",
    "            player2_win += 1\n",
    "        if i % print_every_n == 0:\n",
    "            print('Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f' % (i, player1_win / i, player2_win / i))\n",
    "        player2.backup()\n",
    "        judger.reset()\n",
    "        with open('policy_human_train.bin', 'wb') as f:\n",
    "            pickle.dump(player2.estimations, f)\n",
    "\n",
    "ai_against_human(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Exercise1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
