{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMAwG-eF3WJL"
   },
   "source": [
    "# Exercise II: Cart Pole preliminaries and Monte Carlo\n",
    "Open Files/A2_... and study it in Colab by running it.\n",
    "\n",
    "Observe how all the facets of a reinforcement learning coupled machine/environment system are present.\n",
    "\n",
    "The notebook includes some code to show how the behaviour of the agent can be rendered, using a random policy that exploits the .sample() method.\n",
    "## A2_CartPoleWithRendering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Afc0XGfP3SVP"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IIv-WuaB386T"
   },
   "outputs": [],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HGFYRUgR4UEp"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o32kSv-14hZi",
    "outputId": "6af20bd2-180d-43b6-c94c-e531a3ad6e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 0 action= 0\n",
      "obs= [ 0.00546224 -0.24165428 -0.02821972  0.26101427] reward= 1.0 done= False info= {}\n",
      "step i 1 action= 1\n",
      "obs= [ 0.00062915 -0.0461411  -0.02299944 -0.04043424] reward= 1.0 done= False info= {}\n",
      "step i 2 action= 0\n",
      "obs= [-0.00029367 -0.24092582 -0.02380812  0.24490433] reward= 1.0 done= False info= {}\n",
      "step i 3 action= 1\n",
      "obs= [-0.00511219 -0.04547205 -0.01891004 -0.05519216] reward= 1.0 done= False info= {}\n",
      "step i 4 action= 1\n",
      "obs= [-0.00602163  0.14991587 -0.02001388 -0.35378086] reward= 1.0 done= False info= {}\n",
      "step i 5 action= 1\n",
      "obs= [-0.00302331  0.3453166  -0.0270895  -0.65270695] reward= 1.0 done= False info= {}\n",
      "step i 6 action= 1\n",
      "obs= [ 0.00388302  0.54080511 -0.04014364 -0.95379546] reward= 1.0 done= False info= {}\n",
      "step i 7 action= 1\n",
      "obs= [ 0.01469912  0.73644349 -0.05921955 -1.25881566] reward= 1.0 done= False info= {}\n",
      "step i 8 action= 0\n",
      "obs= [ 0.02942799  0.54212715 -0.08439586 -0.98525248] reward= 1.0 done= False info= {}\n",
      "step i 9 action= 1\n",
      "obs= [ 0.04027054  0.73827191 -0.10410091 -1.30320537] reward= 1.0 done= False info= {}\n",
      "step i 10 action= 0\n",
      "obs= [ 0.05503598  0.54461299 -0.13016502 -1.04484   ] reward= 1.0 done= False info= {}\n",
      "step i 11 action= 1\n",
      "obs= [ 0.06592823  0.7412001  -0.15106182 -1.37538666] reward= 1.0 done= False info= {}\n",
      "step i 12 action= 0\n",
      "obs= [ 0.08075224  0.54825378 -0.17856955 -1.13350659] reward= 1.0 done= False info= {}\n",
      "step i 13 action= 1\n",
      "obs= [ 0.09171731  0.74520527 -0.20123968 -1.47645705] reward= 1.0 done= False info= {}\n",
      "step i 14 action= 1\n",
      "obs= [ 0.10662142  0.94213519 -0.23076882 -1.82465666] reward= 1.0 done= True info= {}\n",
      "Iterations that were run: 14\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "for i in range(50000):\n",
    "  action = env.action_space.sample()\n",
    "  print(\"step i\",i,\"action=\",action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
    "\n",
    "  if done:\n",
    "    break\n",
    "    \n",
    "env.close()\n",
    "print(\"Iterations that were run:\",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yUq_4S_42u2"
   },
   "source": [
    "##Exercise 1:\n",
    "> Can you design a dynamic programming based policy for the agent as in assignment 1? If so, design it and demonstrate that it solves the cart pole problem.\n",
    "\n",
    "No. Because dynamic programming needs to have the complete knowledge of the environment which is *p(s',r|s,a)*. However, for the Cart Pole environment, we don't have the probabilities of environment transition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaIVFkjG-Wl2"
   },
   "source": [
    "##Exercise 2:\n",
    "\n",
    ">Can you design a Monte Carlo based policy for the agent? What ingredients do you require? Explain the design flow, and execute it. Show that it works, or indicate why you can't proceed.\n",
    "\n",
    "Yes. To design a Monte Carlo based policy we require to generate episodes to explore the environment. We first need to generate a finite number of discrete states. Because the state observation of the environment is four continuous variables. Then we can implement on-policy first-visit Monte Carlo control(for $\\epsilon$-*soft* policies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "ZFhQbZuARBJk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Type: Box(4)\n",
    "  Num     Observation               Min                     Max\n",
    "  0       Cart Position             -4.8                    4.8\n",
    "  1       Cart Velocity             -Inf                    Inf\n",
    "  2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "  3       Pole Angular Velocity     -Inf                    Inf\n",
    "'''\n",
    "\n",
    "def discretize_state(obs):\n",
    "\t# env.observation_space.high\n",
    "\t# [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
    "\t# env.observation_space.low\n",
    "\t# [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
    "  discrete = [np.digitize(obs[i], bins) for i, bins in enumerate([\n",
    "    np.linspace(-4.8, 4.8, 9),\n",
    "    np.linspace(-4, 4, 7),\n",
    "    np.linspace(-0.418, 0.418, 9),\n",
    "    np.linspace(-4, 4, 7),\n",
    "    ])]\n",
    "  return ((obs > 0) * 8 **np.arange(len(obs))).sum()\n",
    "\n",
    "def init_policy(epsilon,S,A):\n",
    "  pi_init = np.random.random([S,A])\n",
    "  out = np.zeros_like(pi_init, dtype = np.float)\n",
    "  idx = pi_init.argmax(axis=1)\n",
    "  out[np.arange(S), idx] = 1\n",
    "  pi = out*(1 - epsilon) + epsilon / A\n",
    "  return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "lCTiHnxQ8kWK"
   },
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, actions):\n",
    "  episode = []\n",
    "  obs = env.reset()\n",
    "  i = 0\n",
    "  while True:\n",
    "    i += 1\n",
    "    state = discretize_state(obs)\n",
    "    action = np.random.choice(actions, p=policy[state])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode.append((state, action, reward))\n",
    "    if done:\n",
    "      break\n",
    "  return episode,i\n",
    "\n",
    "\n",
    "def Monte_Carlo(env,gamma,epsilon):\n",
    "  S = 8**len(obs)\n",
    "  A = env.action_space.n\n",
    "  #initialize\n",
    "  pi = init_policy(epsilon,S,A)\n",
    "  states = np.arange(S)\n",
    "  actions = np.arange(A)\n",
    "  Q = np.random.random([S,A])\n",
    "  returns = collections.defaultdict(lambda : collections.defaultdict(list))\n",
    "\n",
    "  for episode in range(1000):\n",
    "    #generate episodes\n",
    "    episode, i = generate_episode(env,pi,actions)\n",
    "    state_actions = [(s, a) for (s,a,r) in episode]\n",
    "    \n",
    "    G = 0\n",
    "    for t in range(i-1,-1,-1):\n",
    "      state, action, reward = episode[t]\n",
    "      G = gamma * G + reward\n",
    "      if not (state,action) in state_actions[0:t]:\n",
    "        returns[state][action] = G\n",
    "        Q[state, action] = np.mean(returns[state][action])\n",
    "        best_action = np.argmax(Q[state])\n",
    "        for a in range(A):\n",
    "          if a == best_action:\n",
    "            pi[state,a] = 1 - epsilon + epsilon / A\n",
    "          else:\n",
    "            pi[state,a] = epsilon / A\n",
    "  return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "8vqCjXub8zEH"
   },
   "outputs": [],
   "source": [
    "Q, policy = Monte_Carlo(env,0.99,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pm0Eoy9SPs3w",
    "outputId": "1ea09828-2159-4cbb-dfaa-ca4e8a891c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 0 action= 0\n",
      "obs= [ 0.04037584 -0.18818377  0.02776897  0.26090951] reward= 1.0 done= False info= {}\n",
      "step i 1 action= 1\n",
      "obs= [ 0.03661217  0.006531    0.03298716 -0.02288701] reward= 1.0 done= False info= {}\n",
      "step i 2 action= 0\n",
      "obs= [ 0.03674279 -0.18904811  0.03252942  0.28001852] reward= 1.0 done= False info= {}\n",
      "step i 3 action= 1\n",
      "obs= [ 0.03296182  0.00559508  0.03812979 -0.00222984] reward= 1.0 done= False info= {}\n",
      "step i 4 action= 0\n",
      "obs= [ 0.03307373 -0.19005238  0.03808519  0.30223548] reward= 1.0 done= False info= {}\n",
      "step i 5 action= 1\n",
      "obs= [0.02927268 0.00450666 0.0441299  0.02180289] reward= 1.0 done= False info= {}\n",
      "step i 6 action= 1\n",
      "obs= [ 0.02936281  0.19896887  0.04456596 -0.25663647] reward= 1.0 done= False info= {}\n",
      "step i 7 action= 0\n",
      "obs= [0.03334219 0.00323991 0.03943323 0.04976352] reward= 1.0 done= False info= {}\n",
      "step i 8 action= 1\n",
      "obs= [ 0.03340699  0.19777489  0.0404285  -0.23022174] reward= 1.0 done= False info= {}\n",
      "step i 9 action= 0\n",
      "obs= [0.03736248 0.00209925 0.03582406 0.07493463] reward= 1.0 done= False info= {}\n",
      "step i 10 action= 1\n",
      "obs= [ 0.03740447  0.1966898   0.03732276 -0.2062339 ] reward= 1.0 done= False info= {}\n",
      "step i 11 action= 0\n",
      "obs= [0.04133827 0.00105456 0.03319808 0.09798493] reward= 1.0 done= False info= {}\n",
      "step i 12 action= 1\n",
      "obs= [ 0.04135936  0.19568538  0.03515778 -0.18404198] reward= 1.0 done= False info= {}\n",
      "step i 13 action= 0\n",
      "obs= [4.52730645e-02 7.84721562e-05 3.14769374e-02 1.19521315e-01] reward= 1.0 done= False info= {}\n",
      "step i 14 action= 1\n",
      "obs= [ 0.04527463  0.19473563  0.03386736 -0.16306709] reward= 1.0 done= False info= {}\n",
      "step i 15 action= 0\n",
      "obs= [ 0.04916935 -0.00085437  0.03060602  0.14010477] reward= 1.0 done= False info= {}\n",
      "step i 16 action= 1\n",
      "obs= [ 0.04915226  0.19381616  0.03340812 -0.14276751] reward= 1.0 done= False info= {}\n",
      "step i 17 action= 0\n",
      "obs= [ 0.05302858 -0.00176792  0.03055277  0.16026507] reward= 1.0 done= False info= {}\n",
      "step i 18 action= 1\n",
      "obs= [ 0.05299322  0.1929036   0.03375807 -0.12262482] reward= 1.0 done= False info= {}\n",
      "step i 19 action= 0\n",
      "obs= [ 0.0568513  -0.00268532  0.03130557  0.1805144 ] reward= 1.0 done= False info= {}\n",
      "step i 20 action= 1\n",
      "obs= [ 0.05679759  0.191975    0.03491586 -0.10213067] reward= 1.0 done= False info= {}\n",
      "step i 21 action= 0\n",
      "obs= [ 0.06063709 -0.0036295   0.03287325  0.20136041] reward= 1.0 done= False info= {}\n",
      "step i 22 action= 1\n",
      "obs= [ 0.0605645   0.19100726  0.03690045 -0.08077399] reward= 1.0 done= False info= {}\n",
      "step i 23 action= 0\n",
      "obs= [ 0.06438464 -0.0046237   0.03528498  0.22331896] reward= 1.0 done= False info= {}\n",
      "step i 24 action= 1\n",
      "obs= [ 0.06429217  0.18997663  0.03975135 -0.05802821] reward= 1.0 done= False info= {}\n",
      "step i 25 action= 0\n",
      "obs= [ 0.0680917  -0.00569207  0.03859079  0.2469268 ] reward= 1.0 done= False info= {}\n",
      "step i 26 action= 1\n",
      "obs= [ 0.06797786  0.18885809  0.04352933 -0.03333829] reward= 1.0 done= False info= {}\n",
      "step i 27 action= 0\n",
      "obs= [ 0.07175502 -0.00686019  0.04286256  0.27275461] reward= 1.0 done= False info= {}\n",
      "step i 28 action= 1\n",
      "obs= [ 0.07161782  0.18762479  0.04831765 -0.00610718] reward= 1.0 done= False info= {}\n",
      "step i 29 action= 0\n",
      "obs= [ 0.07537032 -0.00815561  0.04819551  0.30142055] reward= 1.0 done= False info= {}\n",
      "step i 30 action= 1\n",
      "obs= [0.0752072  0.18624747 0.05422392 0.02431853] reward= 1.0 done= False info= {}\n",
      "step i 31 action= 1\n",
      "obs= [ 0.07893215  0.38055156  0.05471029 -0.25077522] reward= 1.0 done= False info= {}\n",
      "step i 32 action= 0\n",
      "obs= [0.08654318 0.18469276 0.04969479 0.05865021] reward= 1.0 done= False info= {}\n",
      "step i 33 action= 1\n",
      "obs= [ 0.09023704  0.37906826  0.05086779 -0.21794884] reward= 1.0 done= False info= {}\n",
      "step i 34 action= 0\n",
      "obs= [0.0978184  0.18325745 0.04650881 0.09033614] reward= 1.0 done= False info= {}\n",
      "step i 35 action= 1\n",
      "obs= [ 0.10148355  0.37768297  0.04831554 -0.18731813] reward= 1.0 done= False info= {}\n",
      "step i 36 action= 0\n",
      "obs= [0.10903721 0.18190425 0.04456917 0.1202065 ] reward= 1.0 done= False info= {}\n",
      "step i 37 action= 1\n",
      "obs= [ 0.1126753   0.37636025  0.0469733  -0.15808884] reward= 1.0 done= False info= {}\n",
      "step i 38 action= 0\n",
      "obs= [0.1202025  0.18059835 0.04381153 0.14903518] reward= 1.0 done= False info= {}\n",
      "step i 39 action= 1\n",
      "obs= [ 0.12381447  0.37506643  0.04679223 -0.12951057] reward= 1.0 done= False info= {}\n",
      "step i 40 action= 0\n",
      "obs= [0.1313158  0.1793065  0.04420202 0.17755982] reward= 1.0 done= False info= {}\n",
      "step i 41 action= 1\n",
      "obs= [ 0.13490193  0.37376893  0.04775322 -0.10085775] reward= 1.0 done= False info= {}\n",
      "step i 42 action= 0\n",
      "obs= [0.14237731 0.17799628 0.04573606 0.20650057] reward= 1.0 done= False info= {}\n",
      "step i 43 action= 1\n",
      "obs= [ 0.14593723  0.37243539  0.04986607 -0.07141139] reward= 1.0 done= False info= {}\n",
      "step i 44 action= 0\n",
      "obs= [0.15338594 0.17663531 0.04843784 0.23657819] reward= 1.0 done= False info= {}\n",
      "step i 45 action= 1\n",
      "obs= [ 0.15691865  0.37103298  0.05316941 -0.04044114] reward= 1.0 done= False info= {}\n",
      "step i 46 action= 0\n",
      "obs= [0.16433931 0.17519049 0.05236059 0.26853189] reward= 1.0 done= False info= {}\n",
      "step i 47 action= 1\n",
      "obs= [ 0.16784312  0.36952762  0.05773122 -0.00718731] reward= 1.0 done= False info= {}\n",
      "step i 48 action= 0\n",
      "obs= [0.17523367 0.17362728 0.05758748 0.3031372 ] reward= 1.0 done= False info= {}\n",
      "step i 49 action= 1\n",
      "obs= [0.17870621 0.36788324 0.06365022 0.02915765] reward= 1.0 done= False info= {}\n",
      "step i 50 action= 1\n",
      "obs= [ 0.18606388  0.5620374   0.06423337 -0.24278331] reward= 1.0 done= False info= {}\n",
      "step i 51 action= 0\n",
      "obs= [0.19730463 0.36605955 0.05937771 0.06944885] reward= 1.0 done= False info= {}\n",
      "step i 52 action= 1\n",
      "obs= [ 0.20462582  0.56028217  0.06076668 -0.20392486] reward= 1.0 done= False info= {}\n",
      "step i 53 action= 0\n",
      "obs= [0.21583146 0.36434622 0.05668819 0.10729102] reward= 1.0 done= False info= {}\n",
      "step i 54 action= 1\n",
      "obs= [ 0.22311839  0.55861193  0.05883401 -0.16698205] reward= 1.0 done= False info= {}\n",
      "step i 55 action= 0\n",
      "obs= [0.23429062 0.36269932 0.05549437 0.14366562] reward= 1.0 done= False info= {}\n",
      "step i 56 action= 1\n",
      "obs= [ 0.24154461  0.55698441  0.05836768 -0.13100642] reward= 1.0 done= False info= {}\n",
      "step i 57 action= 0\n",
      "obs= [0.2526843  0.36107704 0.05574755 0.17950457] reward= 1.0 done= False info= {}\n",
      "step i 58 action= 1\n",
      "obs= [ 0.25990584  0.55535876  0.05933764 -0.09508399] reward= 1.0 done= False info= {}\n",
      "step i 59 action= 0\n",
      "obs= [0.27101302 0.35943876 0.05743596 0.21571383] reward= 1.0 done= False info= {}\n",
      "step i 60 action= 1\n",
      "obs= [ 0.27820179  0.55369457  0.06175024 -0.05831251] reward= 1.0 done= False info= {}\n",
      "step i 61 action= 0\n",
      "obs= [0.28927568 0.35774409 0.06058399 0.25319604] reward= 1.0 done= False info= {}\n",
      "step i 62 action= 1\n",
      "obs= [ 0.29643056  0.55195099  0.06564791 -0.01977907] reward= 1.0 done= False info= {}\n",
      "step i 63 action= 0\n",
      "obs= [0.30746958 0.355952   0.06525233 0.29287276] reward= 1.0 done= False info= {}\n",
      "step i 64 action= 1\n",
      "obs= [0.31458862 0.55008585 0.07110978 0.02146228] reward= 1.0 done= False info= {}\n",
      "step i 65 action= 1\n",
      "obs= [ 0.32559034  0.74411978  0.07153903 -0.24796439] reward= 1.0 done= False info= {}\n",
      "step i 66 action= 0\n",
      "obs= [0.34047274 0.54805294 0.06657974 0.06639816] reward= 1.0 done= False info= {}\n",
      "step i 67 action= 1\n",
      "obs= [ 0.3514338   0.74216028  0.0679077  -0.20455777] reward= 1.0 done= False info= {}\n",
      "step i 68 action= 0\n",
      "obs= [0.366277   0.54613623 0.06381655 0.10875012] reward= 1.0 done= False info= {}\n",
      "step i 69 action= 1\n",
      "obs= [ 0.37719973  0.7402884   0.06599155 -0.16313599] reward= 1.0 done= False info= {}\n",
      "step i 70 action= 0\n",
      "obs= [0.39200549 0.54428686 0.06272883 0.14961383] reward= 1.0 done= False info= {}\n",
      "step i 71 action= 1\n",
      "obs= [ 0.40289123  0.73845708  0.06572111 -0.12263848] reward= 1.0 done= False info= {}\n",
      "step i 72 action= 0\n",
      "obs= [0.41766037 0.54245812 0.06326834 0.19003336] reward= 1.0 done= False info= {}\n",
      "step i 73 action= 1\n",
      "obs= [ 0.42850953  0.73662057  0.06706901 -0.08203911] reward= 1.0 done= False info= {}\n",
      "step i 74 action= 0\n",
      "obs= [0.44324195 0.54060449 0.06542822 0.23102746] reward= 1.0 done= False info= {}\n",
      "step i 75 action= 1\n",
      "obs= [ 0.45405404  0.73473347  0.07004877 -0.04032077] reward= 1.0 done= False info= {}\n",
      "step i 76 action= 0\n",
      "obs= [0.46874871 0.53868058 0.06924236 0.27361485] reward= 1.0 done= False info= {}\n",
      "step i 77 action= 1\n",
      "obs= [0.47952232 0.73274975 0.07471465 0.00354967] reward= 1.0 done= False info= {}\n",
      "step i 78 action= 1\n",
      "obs= [ 0.49417731  0.92672508  0.07478565 -0.26465592] reward= 1.0 done= False info= {}\n",
      "step i 79 action= 0\n",
      "obs= [0.51271181 0.7306198  0.06949253 0.05064629] reward= 1.0 done= False info= {}\n",
      "step i 80 action= 1\n",
      "obs= [ 0.52732421  0.92468004  0.07050546 -0.21932713] reward= 1.0 done= False info= {}\n",
      "step i 81 action= 0\n",
      "obs= [0.54581781 0.72862479 0.06611891 0.09473654] reward= 1.0 done= False info= {}\n",
      "step i 82 action= 1\n",
      "obs= [ 0.56039031  0.92273983  0.06801364 -0.176375  ] reward= 1.0 done= False info= {}\n",
      "step i 83 action= 0\n",
      "obs= [0.5788451  0.72671378 0.06448614 0.13696485] reward= 1.0 done= False info= {}\n",
      "step i 84 action= 1\n",
      "obs= [ 0.59337938  0.92085566  0.06722544 -0.13469689] reward= 1.0 done= False info= {}\n",
      "step i 85 action= 0\n",
      "obs= [0.61179649 0.72483841 0.0645315  0.17841425] reward= 1.0 done= False info= {}\n",
      "step i 86 action= 1\n",
      "obs= [ 0.62629326  0.91898038  0.06809979 -0.09323347] reward= 1.0 done= False info= {}\n",
      "step i 87 action= 0\n",
      "obs= [0.64467287 0.72295179 0.06623512 0.22013372] reward= 1.0 done= False info= {}\n",
      "step i 88 action= 1\n",
      "obs= [ 0.6591319   0.91706751  0.07063779 -0.05094251] reward= 1.0 done= False info= {}\n",
      "step i 89 action= 0\n",
      "obs= [0.67747325 0.72100752 0.06961894 0.26316432] reward= 1.0 done= False info= {}\n",
      "step i 90 action= 1\n",
      "obs= [ 0.6918934   0.91507024  0.07488223 -0.00677318] reward= 1.0 done= False info= {}\n",
      "step i 91 action= 1\n",
      "obs= [ 0.71019481  1.10904285  0.07474677 -0.27492191] reward= 1.0 done= False info= {}\n",
      "step i 92 action= 0\n",
      "obs= [0.73237566 0.91293844 0.06924833 0.04036845] reward= 1.0 done= False info= {}\n",
      "step i 93 action= 1\n",
      "obs= [ 0.75063443  1.10700256  0.0700557  -0.22968733] reward= 1.0 done= False info= {}\n",
      "step i 94 action= 0\n",
      "obs= [0.77277448 0.91095309 0.06546195 0.08424508] reward= 1.0 done= False info= {}\n",
      "step i 95 action= 1\n",
      "obs= [ 0.79099355  1.10507857  0.06714685 -0.18708738] reward= 1.0 done= False info= {}\n",
      "step i 96 action= 0\n",
      "obs= [0.81309512 0.90906338 0.0634051  0.12599916] reward= 1.0 done= False info= {}\n",
      "step i 97 action= 1\n",
      "obs= [ 0.83127639  1.10322239  0.06592509 -0.14602552] reward= 1.0 done= False info= {}\n",
      "step i 98 action= 0\n",
      "obs= [0.85334083 0.90722133 0.06300458 0.16670537] reward= 1.0 done= False info= {}\n",
      "step i 99 action= 1\n",
      "obs= [ 0.87148526  1.10138747  0.06633868 -0.10545488] reward= 1.0 done= False info= {}\n",
      "step i 100 action= 0\n",
      "obs= [0.89351301 0.90538066 0.06422959 0.2073979 ] reward= 1.0 done= False info= {}\n",
      "step i 101 action= 1\n",
      "obs= [ 0.91162062  1.09952817  0.06837754 -0.06435234] reward= 1.0 done= False info= {}\n",
      "step i 102 action= 0\n",
      "obs= [0.93361119 0.90349587 0.0670905  0.2490963 ] reward= 1.0 done= False info= {}\n",
      "step i 103 action= 1\n",
      "obs= [ 0.9516811   1.09759876  0.07207242 -0.0216932 ] reward= 1.0 done= False info= {}\n",
      "step i 104 action= 0\n",
      "obs= [0.97363308 0.90152119 0.07163856 0.29283055] reward= 1.0 done= False info= {}\n",
      "step i 105 action= 1\n",
      "obs= [0.9916635  1.0955525  0.07749517 0.02357383] reward= 1.0 done= False info= {}\n",
      "step i 106 action= 1\n",
      "obs= [ 1.01357455  1.28948249  0.07796665 -0.24368733] reward= 1.0 done= False info= {}\n",
      "step i 107 action= 0\n",
      "obs= [1.0393642  1.09333849 0.0730929  0.07253386] reward= 1.0 done= False info= {}\n",
      "step i 108 action= 1\n",
      "obs= [ 1.06123097  1.28734058  0.07454358 -0.19622209] reward= 1.0 done= False info= {}\n",
      "step i 109 action= 0\n",
      "obs= [1.08697778 1.09123591 0.07061914 0.11901355] reward= 1.0 done= False info= {}\n",
      "step i 110 action= 1\n",
      "obs= [ 1.1088025   1.28527873  0.07299941 -0.15058043] reward= 1.0 done= False info= {}\n",
      "step i 111 action= 0\n",
      "obs= [1.13450808 1.08919151 0.0699878  0.16420981] reward= 1.0 done= False info= {}\n",
      "step i 112 action= 1\n",
      "obs= [ 1.15629191  1.28324541  0.07327199 -0.10559881] reward= 1.0 done= False info= {}\n",
      "step i 113 action= 0\n",
      "obs= [1.18195681 1.08715408 0.07116002 0.20927166] reward= 1.0 done= False info= {}\n",
      "step i 114 action= 1\n",
      "obs= [ 1.2036999   1.2811902   0.07534545 -0.06014253] reward= 1.0 done= False info= {}\n",
      "step i 115 action= 0\n",
      "obs= [1.2293237  1.08507334 0.0741426  0.25532875] reward= 1.0 done= False info= {}\n",
      "step i 116 action= 1\n",
      "obs= [ 1.25102517  1.2790627   0.07924918 -0.0130779 ] reward= 1.0 done= False info= {}\n",
      "step i 117 action= 0\n",
      "obs= [1.27660642 1.08289892 0.07898762 0.30351913] reward= 1.0 done= False info= {}\n",
      "step i 118 action= 1\n",
      "obs= [1.2982644  1.27681153 0.085058   0.03675533] reward= 1.0 done= False info= {}\n",
      "step i 119 action= 1\n",
      "obs= [ 1.32380063  1.47061726  0.08579311 -0.22792537] reward= 1.0 done= False info= {}\n",
      "step i 120 action= 0\n",
      "obs= [1.35321298 1.27438071 0.0812346  0.09053907] reward= 1.0 done= False info= {}\n",
      "step i 121 action= 1\n",
      "obs= [ 1.37870059  1.46825     0.08304538 -0.17544916] reward= 1.0 done= False info= {}\n",
      "step i 122 action= 0\n",
      "obs= [1.40806559 1.2720438  0.0795364  0.14223315] reward= 1.0 done= False info= {}\n",
      "step i 123 action= 1\n",
      "obs= [ 1.43350647  1.4659419   0.08238106 -0.12433546] reward= 1.0 done= False info= {}\n",
      "step i 124 action= 0\n",
      "obs= [1.4628253  1.26974227 0.07989435 0.19315854] reward= 1.0 done= False info= {}\n",
      "step i 125 action= 1\n",
      "obs= [ 1.48822015  1.46363583  0.08375752 -0.0732901 ] reward= 1.0 done= False info= {}\n",
      "step i 126 action= 0\n",
      "obs= [1.51749287 1.2674192  0.08229172 0.24459897] reward= 1.0 done= False info= {}\n",
      "step i 127 action= 1\n",
      "obs= [ 1.54284125  1.46127525  0.0871837  -0.0210346 ] reward= 1.0 done= False info= {}\n",
      "step i 128 action= 0\n",
      "obs= [1.57206675 1.2650182  0.08676301 0.29783242] reward= 1.0 done= False info= {}\n",
      "step i 129 action= 1\n",
      "obs= [1.59736712 1.4588031  0.09271966 0.0337248 ] reward= 1.0 done= False info= {}\n",
      "step i 130 action= 1\n",
      "obs= [ 1.62654318  1.65248154  0.09339415 -0.22832444] reward= 1.0 done= False info= {}\n",
      "step i 131 action= 0\n",
      "obs= [1.65959281 1.45615764 0.08882766 0.09229601] reward= 1.0 done= False info= {}\n",
      "step i 132 action= 1\n",
      "obs= [ 1.68871596  1.64990154  0.09067358 -0.17109306] reward= 1.0 done= False info= {}\n",
      "step i 133 action= 0\n",
      "obs= [1.72171399 1.45360666 0.08725172 0.1487612 ] reward= 1.0 done= False info= {}\n",
      "step i 134 action= 1\n",
      "obs= [ 1.75078613  1.64737785  0.09022695 -0.11517046] reward= 1.0 done= False info= {}\n",
      "step i 135 action= 0\n",
      "obs= [1.78373368 1.4510867  0.08792354 0.20455934] reward= 1.0 done= False info= {}\n",
      "step i 136 action= 1\n",
      "obs= [ 1.81275542  1.64484845  0.09201472 -0.05914437] reward= 1.0 done= False info= {}\n",
      "step i 137 action= 0\n",
      "obs= [1.84565239 1.4485359  0.09083184 0.26109291] reward= 1.0 done= False info= {}\n",
      "step i 138 action= 1\n",
      "obs= [ 1.87462311e+00  1.64225175e+00  9.60536941e-02 -1.61515843e-03] reward= 1.0 done= False info= {}\n",
      "step i 139 action= 0\n",
      "obs= [1.90746814 1.44589286 0.09602139 0.31976185] reward= 1.0 done= False info= {}\n",
      "step i 140 action= 1\n",
      "obs= [1.936386   1.63952551 0.10241663 0.05883777] reward= 1.0 done= False info= {}\n",
      "step i 141 action= 1\n",
      "obs= [ 1.96917651  1.83304125  0.10359338 -0.19985693] reward= 1.0 done= False info= {}\n",
      "step i 142 action= 0\n",
      "obs= [2.00583733 1.63660195 0.09959624 0.12362436] reward= 1.0 done= False info= {}\n",
      "step i 143 action= 1\n",
      "obs= [ 2.03856937  1.83016643  0.10206873 -0.1360506 ] reward= 1.0 done= False info= {}\n",
      "step i 144 action= 0\n",
      "obs= [2.0751727  1.63374186 0.09934772 0.18700894] reward= 1.0 done= False info= {}\n",
      "step i 145 action= 1\n",
      "obs= [ 2.10784754  1.82731243  0.1030879  -0.07275497] reward= 1.0 done= False info= {}\n",
      "step i 146 action= 0\n",
      "obs= [2.14439379 1.63087523 0.1016328  0.25059072] reward= 1.0 done= False info= {}\n",
      "step i 147 action= 1\n",
      "obs= [ 2.17701129  1.82441008  0.10664461 -0.00838492] reward= 1.0 done= False info= {}\n",
      "step i 148 action= 0\n",
      "obs= [2.21349949 1.62793316 0.10647692 0.31595027] reward= 1.0 done= False info= {}\n",
      "step i 149 action= 1\n",
      "obs= [2.24605816 1.82139006 0.11279592 0.05865343] reward= 1.0 done= False info= {}\n",
      "step i 150 action= 1\n",
      "obs= [ 2.28248596  2.01472919  0.11396899 -0.19642062] reward= 1.0 done= False info= {}\n",
      "step i 151 action= 0\n",
      "obs= [2.32278054 1.81817715 0.11004058 0.12992915] reward= 1.0 done= False info= {}\n",
      "step i 152 action= 1\n",
      "obs= [ 2.35914408  2.01156482  0.11263916 -0.12611116] reward= 1.0 done= False info= {}\n",
      "step i 153 action= 0\n",
      "obs= [2.39937538 1.81502452 0.11011694 0.19987698] reward= 1.0 done= False info= {}\n",
      "step i 154 action= 1\n",
      "obs= [ 2.43567587  2.00841336  0.11411448 -0.05614033] reward= 1.0 done= True info= {}\n"
     ]
    }
   ],
   "source": [
    "# Convert epsilon-soft policy to a greedy policy\n",
    "S = 8**len(obs)\n",
    "A = env.action_space.n\n",
    "pi_greedy = np.zeros_like(policy, dtype=np.float)\n",
    "pi_greedy[np.arange(S), np.argmax(policy, axis=1)] = 1\n",
    "\n",
    "# Test\n",
    "done = False\n",
    "obs = env.reset()\n",
    "for i in range(50000):\n",
    "    state = discretize_state(obs)\n",
    "    action = np.random.choice(np.arange(A), p=policy[state])\n",
    "\n",
    "    print(\"step i\",i,\"action=\",action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Exercise2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
